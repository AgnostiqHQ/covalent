{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classifier tutorial\n",
    "\n",
    "MNIST database of handwritten digits is a popular dataset to demonstrate machine learning classifiers. The dataset is comprised of handwritten digits with the corresponding ground truth label. In this tutorial (based on [1]), we train a basic Neural Network (NN) classifier using PyTorch. \n",
    "\n",
    "A sample of the MNIST dataset:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./assets/mnist_examples.png\" style=\"width: 45%; height: 45%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The tutorial has two main parts: \n",
    "\n",
    "- First, a \"normal\" workflow function (without using Covalent) is defined to train the MNIST classifier.\n",
    "\n",
    "- Second, this workflow is converted into a Covalent workflow, which is then \"dispatched\" for execution. \n",
    "\n",
    "Lastly, we review the key benefits that are unlocked when transforming a \"normal\" workflow with Covalent. One major advantage of a Covalent workflow is that the task dependencies and execution details can be tracked easily in the Covalent user interface (UI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "This tutorial requires installing Covalent, PyTorch and Torchvision. Uncomment and run the following cell to install the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covalent\n",
      "torch==1.13.1\n",
      "torchvision==0.14.1\n"
     ]
    }
   ],
   "source": [
    "with open(\"./requirements.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        print(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covalent:  Covalent Workflow CLI Tool\r\n",
      "Copyright (C) 2021 Agnostiq Inc.\r\n",
      "Built using Python 3.8 (Platform: x86_64-linux)\r\n",
      "Release version 0.209.1rc0\r\n"
     ]
    }
   ],
   "source": [
    "!covalent --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required packages\n",
    "# !pip install -r ./requirements.txt\n",
    "\n",
    "# In some cases, the following two lines might also need to be run\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once Covalent has been installed, run `covalent start` in a terminal to start the Covalent server. Also, `covalent status / stop` are used to check the server status / stop the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Covalent, PyTorch and other relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covalent as ct\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct MNIST classifier training workflow (without Covalent)\n",
    "\n",
    "Construct a convolutional neural network model by inheriting from `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a data loader to retrieve the classifier training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(\n",
    "    batch_size: int,\n",
    "    train: bool,\n",
    "    download: bool = True,\n",
    "    shuffle: bool = True,\n",
    "    data_dir: str = \"~/data/mnist/\",\n",
    ") -> torch.utils.data.dataloader.DataLoader:\n",
    "    \"\"\"MNIST data loader.\"\"\"\n",
    "\n",
    "    data_dir = Path(data_dir).expanduser()\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data = datasets.MNIST(data_dir, train=train, download=download, transform=ToTensor())\n",
    "\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a function to retrieve a Stochastic Gradient Descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(\n",
    "    model: NeuralNetwork, learning_rate: float, momentum: float\n",
    ") -> torch.optim.Optimizer:\n",
    "    \"\"\"Get Stochastic Gradient Descent optimizer.\"\"\"\n",
    "\n",
    "    return torch.optim.SGD(model.parameters(), learning_rate, momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to train the model over one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_over_one_epoch(\n",
    "    dataloader: torch.utils.data.dataloader.DataLoader,\n",
    "    model: NeuralNetwork,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    log_interval: int,\n",
    "    epoch: int,\n",
    "    loss_fn,\n",
    "    train_losses: list,\n",
    "    train_counter: int,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"Train neural network model over one epoch.\"\"\"\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % log_interval == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "            train_losses.append(loss)\n",
    "            train_counter.append((batch * 64) + ((epoch - 1) * len(dataloader.dataset)))\n",
    "\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to test the performance of the classifier for a given loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    dataloader: torch.utils.data.dataloader.DataLoader,\n",
    "    model: NeuralNetwork,\n",
    "    loss_fn: callable,\n",
    "    device: str = \"cpu\",\n",
    ") -> None:\n",
    "    \"\"\"Test the model performance.\"\"\"\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    accuracy = 100*correct\n",
    "    print(f\"Test Error: \\n Accuracy: {(accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return accuracy, test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to train the model over several epochs and save the final state of the _optimizer_ and the _neural network_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_dataloader: torch.utils.data.dataloader.DataLoader,\n",
    "    train_losses: list,\n",
    "    train_counter: int,\n",
    "    log_interval: int,\n",
    "    model: NeuralNetwork,\n",
    "    learning_rate: float,\n",
    "    momentum: float,\n",
    "    loss_fn: callable,\n",
    "    epochs: int,\n",
    "    results_dir: str = \"~/data/mnist/results/\",\n",
    ") -> Tuple[NeuralNetwork,]:\n",
    "    \"\"\"Train neural network model.\"\"\"\n",
    "\n",
    "    results_dir = Path(results_dir).expanduser()\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "        model, optimizer = train_over_one_epoch(\n",
    "            dataloader=train_dataloader,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_losses=train_losses,\n",
    "            train_counter=train_counter,\n",
    "            log_interval=log_interval,\n",
    "            epoch=epoch,\n",
    "            loss_fn=loss_fn,\n",
    "        )\n",
    "\n",
    "    # Save model and optimizer\n",
    "    torch.save(model.state_dict(), f\"{results_dir}model.pth\")\n",
    "    torch.save(optimizer.state_dict(), f\"{results_dir}optimizer.pth\")\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we put all these tasks together to construct the MNIST classifier training and test workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workflow(\n",
    "    model: NeuralNetwork,\n",
    "    epochs: int = 2,\n",
    "    batch_size_train: int = 64,\n",
    "    batch_size_test: int = 1000,\n",
    "    learning_rate: float = 0.01,\n",
    "    momentum: float = 0.5,\n",
    "    log_interval: int = 200,\n",
    "    loss_fn: callable = F.nll_loss,\n",
    "):\n",
    "    \"\"\"MNIST classifier training workflow\"\"\"\n",
    "\n",
    "    train_dataloader = data_loader(batch_size=batch_size_train, train=True)\n",
    "    test_dataloader = data_loader(batch_size=batch_size_test, train=False)\n",
    "\n",
    "    train_losses, train_counter, test_losses = [], [], []\n",
    "    model, optimizer = train_model(\n",
    "        train_dataloader=train_dataloader,\n",
    "        train_losses=train_losses,\n",
    "        train_counter=train_counter,\n",
    "        log_interval=log_interval,\n",
    "        model=model,\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=momentum,\n",
    "        loss_fn=loss_fn,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    accuracy, test_loss = test(dataloader=test_dataloader, model=model, loss_fn=loss_fn)\n",
    "\n",
    "    return accuracy, test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run MNIST classifier workflow as a normal function (without Covalent)\n",
    "\n",
    "Run the MNIST classifier workflow to benchmark the performance and the time taken to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.287198  [    0/60000]\n",
      "loss: 2.241443  [12800/60000]\n",
      "loss: 1.228720  [25600/60000]\n",
      "loss: 0.885399  [38400/60000]\n",
      "loss: 0.771022  [51200/60000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.462313  [    0/60000]\n",
      "loss: 0.655870  [12800/60000]\n",
      "loss: 0.425767  [25600/60000]\n",
      "loss: 0.461888  [38400/60000]\n",
      "loss: 0.181691  [51200/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.185087 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "workflow(\n",
    "    model=NeuralNetwork().to(\"cpu\"),\n",
    ")\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular workflow takes 31.716453075408936 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Regular workflow takes {end - start} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final test accuracy is 93.5% and average loss is 0.211207."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and run workflow with Covalent\n",
    "\n",
    "First, we convert the normal workflow function into a Covalent workflow function.\n",
    "\n",
    "This can be done in two ways. The most convenient method is to add `ct.electron` decorators to each of the task functions defined above and `ct.lattice` decorator to the workflow function.\n",
    "\n",
    "For example,\n",
    "\n",
    "```python\n",
    "@ct.electron\n",
    "def data_loader(...):\n",
    "    ...\n",
    "\n",
    "\n",
    "@ct.electron\n",
    "def train_over_one_epoch(...):\n",
    "    ...\n",
    "\n",
    "\n",
    "@ct.lattice\n",
    "def workflow(...):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Alternatively, we can also use the decorator functions (shown in the cell below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tasks to electrons\n",
    "data_loader = ct.electron(data_loader)\n",
    "get_optimizer = ct.electron(get_optimizer)\n",
    "train_over_one_epoch = ct.electron(train_over_one_epoch)\n",
    "train_model = ct.electron(train_model)\n",
    "test = ct.electron(test)\n",
    "\n",
    "# Convert workflow to lattice\n",
    "workflow = ct.lattice(workflow, executor=\"local\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We use the \"local\" executor in this tutorial because the default Dask-based executor does not yet capture the output of `print` statements. This issue will be addressed in a future release. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Covalent dispatcher (`ct.dispatch`) can be used to dispatch the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch id: ef849866-ed78-49bd-b4da-29265d78966b\n",
      "Covalent workflow takes 0:00:35.537538 seconds.\n"
     ]
    }
   ],
   "source": [
    "dispatch_id = ct.dispatch(workflow)(model=NeuralNetwork().to(\"cpu\"))\n",
    "print(f\"Dispatch id: {dispatch_id}\")\n",
    "result = ct.get_result(dispatch_id=dispatch_id, wait=True)\n",
    "print(f\"Covalent workflow takes {result.end_time - result.start_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lattice Result\n",
      "==============\n",
      "status: POSTPROCESSING_FAILED\n",
      "result: None\n",
      "input args: []\n",
      "input kwargs: {'model': 'NeuralNetwork(\\n  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\\n  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\\n  (conv2_drop): Dropout2d(p=0.5, inplace=False)\\n  (fc1): Linear(in_features=320, out_features=50, bias=True)\\n  (fc2): Linear(in_features=50, out_features=10, bias=True)\\n)'}\n",
      "error: Post-processing failed: Traceback (most recent call last):\n",
      "  File \"/Users/faiyaz/opt/anaconda3/envs/mnist/lib/python3.8/site-packages/covalent/executor/utils/wrappers.py\", line 36, in io_wrapper\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/Users/faiyaz/opt/anaconda3/envs/mnist/lib/python3.8/site-packages/covalent/executor/base.py\", line 92, in wrapper_fn\n",
      "    output = fn(*new_args, **new_kwargs)\n",
      "  File \"/Users/faiyaz/opt/anaconda3/envs/mnist/lib/python3.8/site-packages/covalent_dispatcher/_core/runner.py\", line 433, in _post_process\n",
      "    result = workflow_function(*args, **kwargs)\n",
      "  File \"/var/folders/65/q5vwfnjd4nbgdb735y8qt9yw0000gn/T/ipykernel_76640/486393520.py\", line 28, in workflow\n",
      "ValueError: not enough values to unpack (expected 2, got 0)\n",
      "\n",
      "\n",
      "start_time: 2023-01-13 16:22:56.636010\n",
      "end_time: 2023-01-13 16:23:32.173548\n",
      "\n",
      "results_dir: /Users/faiyaz/.local/share/covalent/data\n",
      "dispatch_id: ef849866-ed78-49bd-b4da-29265d78966b\n",
      "\n",
      "Node Outputs\n",
      "------------\n",
      "data_loader(0): <torch.utils.data.dataloader.DataLoader object at 0x7f88a84ec8e0>\n",
      ":parameter:64(1): 64\n",
      ":parameter:True(2): True\n",
      "data_loader(3): <torch.utils.data.dataloader.DataLoader object at 0x7fec20fff970>\n",
      ":parameter:1000(4): 1000\n",
      ":parameter:False(5): False\n",
      "train_model(6): (NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.5\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      "))\n",
      "_auto_list_node(7): []\n",
      "_auto_list_node(8): []\n",
      ":parameter:200(9): 200\n",
      ":parameter:NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")(10): NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      ":parameter:0.01(11): 0.01\n",
      ":parameter:0.5(12): 0.5\n",
      ":parameter:<function nll_loss at 0x7feb7b134ca0>(13): <function nll_loss at 0x7feb7b134ca0>\n",
      ":parameter:2(14): 2\n",
      ":train_model()[0](15): NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      ":parameter:0(16): 0\n",
      ":train_model()[1](17): SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.5\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      ":parameter:1(18): 1\n",
      "test(19): (94.01, 0.2050844207406044)\n",
      ":parameter:<function nll_loss at 0x7feb7b134ca0>(20): <function nll_loss at 0x7feb7b134ca0>\n",
      ":test()[0](21): 94.01\n",
      ":parameter:0(22): 0\n",
      ":test()[1](23): 0.2050844207406044\n",
      ":parameter:1(24): 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view a summary of the _result_ object, uncomment and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.290228  [    0/60000]\n",
      "loss: 2.272725  [12800/60000]\n",
      "loss: 1.660198  [25600/60000]\n",
      "loss: 0.965520  [38400/60000]\n",
      "loss: 0.900267  [51200/60000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.336234  [    0/60000]\n",
      "loss: 0.765050  [12800/60000]\n",
      "loss: 0.680971  [25600/60000]\n",
      "loss: 0.484477  [38400/60000]\n",
      "loss: 0.369142  [51200/60000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.get_node_result(6)[\"stdout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.205084 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.get_node_result(19)[\"stdout\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the workflow has been dispatched, the results can be tracked on the Covalent UI. Use `covalent status` (shown below) to find the browser address to access the Covalent UI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covalent server is running at http://localhost:48008.\r\n"
     ]
    }
   ],
   "source": [
    "!covalent status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking on `http://localhost:48008` we can see the Covalent UI which lists the various dispatch ids.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./assets/ui_dispatch_ids.png\" style=\"width: 95%; height: 95%\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking on the dispatch id, we can see the details of the workflow execution. Note the task execution dependency graph. \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./assets/ui_workflow.png\" style=\"width: 95%; height: 95%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the progress and status of the tasks (`get_optimizer`, `data_loader` etc.) can also be tracked in the Covalent UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this tutorial, we trained a MNIST classifier using a workflow comprised of several tasks. Then, we converted this workflow into a Covalent workflow. Here, we summarize some of the added benefits of using Covalent.\n",
    "\n",
    "* Covalent allows the user to deploy multiple workflows without having to wait for them to finish running. This makes it very useful for experimentation in a pre-production environment.\n",
    "\n",
    "* A Covalent workflow can be _dispatched_ to take advantage of automatic parallelization, user friendly interface etc. but it can also just as easily be run as a normal Python function. Hence, adding the electron / lattice decorators only enhances what can be done with the workflow with a minimum work. For example, in this workflow the training and testing of dataloaders (I/O bound tasks) are mutually independent and hence would be automatically parallelized.\n",
    "\n",
    "* Execution time for Covalent workflows and tasks are readily available without needing any additional code.\n",
    "\n",
    "* In this tutorial, we saw that the execution time overhead added by Covalent is small (10 seconds) and scales very well as the problems grow in complexity. Particularly for problems that are parallelizable, Covalent becomes a much better alternative than a regular workflow.\n",
    "\n",
    "### References:\n",
    "\n",
    "[1] https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mnist] *",
   "language": "python",
   "name": "conda-env-mnist-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
