{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(\"./assets/requirements.txt\", \"r\") as file:\n","    for line in file:\n","        print(line.rstrip())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install necessary packages\n","# !pip install -r ./requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import covalent as ct\n","import os\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["Seed Torch for reproducibility and set default tensor type"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["GLOBAL_SEED = 1989\n","torch.manual_seed(GLOBAL_SEED)\n","torch.set_default_tensor_type(torch.DoubleTensor)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def generate_normal_time_series_set(\n","    p: int, num_series: int, noise_amp: float, t_init: float, t_end: float, seed: int = GLOBAL_SEED\n",") -> tuple:\n","    \"\"\"Generate a normal time series data set where each of the p elements\n","    is drawn from a normal distribution x_t ~ N(0, noise_amp).\n","    \"\"\"\n","    torch.manual_seed(seed)\n","    X = torch.normal(0, noise_amp, (num_series, p))\n","    T = torch.linspace(t_init, t_end, p)\n","    return X, T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def generate_anomalous_time_series_set(\n","    p: int,\n","    num_series: int,\n","    noise_amp: float,\n","    spike_amp: float,\n","    max_duration: int,\n","    t_init: float,\n","    t_end: float,\n","    seed: int = GLOBAL_SEED,\n",") -> tuple:\n","    \"\"\"Generate an anomalous time series data set where the p elements of each sequence are\n","    from a normal distribution x_t ~ N(0, noise_amp). Then,\n","    anomalous spikes of random amplitudes and durations are inserted.\n","    \"\"\"\n","    torch.manual_seed(seed)\n","    Y = torch.normal(0, noise_amp, (num_series, p))\n","    for y in Y:\n","        # 5–10 spikes allowed\n","        spike_num = torch.randint(low=5, high=10, size=())\n","        durations = torch.randint(low=1, high=max_duration, size=(spike_num,))\n","        spike_start_idxs = torch.randperm(p - max_duration)[:spike_num]\n","        for start_idx, duration in zip(spike_start_idxs, durations):\n","            y[start_idx : start_idx + duration] += torch.normal(0.0, spike_amp, (duration,))\n","    T = torch.linspace(t_init, t_end, p)\n","    return Y, T"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's do a quick sanity check and plot a couple of these series. Despite the<br>\n","above function's ``@ct.electron`` decorators, these can still be used as normal<br>\n","Python functions without using the Covalent server. This is useful<br>\n","for quick checks like this:<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_norm, T_norm = generate_normal_time_series_set(25, 25, 0.1, 0.1, 2 * torch.pi)\n","Y_anom, T_anom = generate_anomalous_time_series_set(25, 25, 0.1, 0.4, 5, 0, 2 * torch.pi)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure()\n","plt.plot(T_norm, X_norm[0], label=\"Normal\")\n","plt.plot(T_anom, Y_anom[1], label=\"Anomalous\")\n","plt.ylabel(\"$y(t)$\")\n","plt.xlabel(\"t\")\n","plt.grid()\n","leg = plt.legend()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Taking a look at the above, the generated series are what we wanted. We have<br>\n","a simple human-parsable notion of what it is for a time series to be anomalous<br>\n","(big spikes). Of course, we don't need a complicated algorithm to be able to detect<br>\n","such anomalies but this is just a didactic example remember!<br>\n","<br>\n","Like many machine learning algorithms, training is done in mini-batches.<br>\n","Examining the form of the loss function<br>\n",":math:`\\mathcal{L}(\\boldsymbol{\\phi})`, we can see that time series are<br>\n","atomized. In other words, each term in the mean square error is for a given<br>\n",":math:`x_t` and not measured against the entire series :math:`x`. This<br>\n","allows us to break down the training set :math:`X` into<br>\n","time-series-independent chunks. Here’s an electron to do that:<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def make_atomized_training_set(X: torch.Tensor, T: torch.Tensor) -> list:\n","    \"\"\"Convert input time series data provided in a two-dimensional tensor format\n","    to atomized tuple chunks: (xt, t).\n","    \"\"\"\n","    X_flat = torch.flatten(X)\n","    T_flat = T.repeat(X.size()[0])\n","    atomized = [(xt, t) for xt, t in zip(X_flat, T_flat)]\n","    return atomized"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We now wish to pass this to a cycled ``torch.utils.data.DataLoader``.<br>\n","However, this object is not<br>\n","`pickleable <https://docs.python.org/3/library/pickle.html#:~:text=%E2%80%9CPickling%E2%80%9D%20is%20the%20process%20whereby,back%20into%20an%20object%20hierarchy.>`__,<br>\n","which is a requirement of electrons in Covalent. We therefore use the<br>\n","below helper class to create a pickleable version.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections.abc import Iterator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DataGetter:\n","    \"\"\"A pickleable mock-up of a Python iterator on a torch.utils.Dataloader.\n","    Provide a dataset X and the resulting object O will allow you to use next(O).\n","    \"\"\"\n","    def __init__(self, X: torch.Tensor, batch_size: int, seed: int = GLOBAL_SEED) -> None:\n","        \"\"\"Calls the _init_data method on intialization of a DataGetter object.\"\"\"\n","        torch.manual_seed(seed)\n","        self.X = X\n","        self.batch_size = batch_size\n","        self.data = []\n","        self._init_data(\n","            iter(torch.utils.data.DataLoader(self.X, batch_size=self.batch_size, shuffle=True))\n","        )\n","    def _init_data(self, iterator: Iterator) -> None:\n","        \"\"\"Load all of the iterator into a list.\"\"\"\n","        x = next(iterator, None)\n","        while x is not None:\n","            self.data.append(x)\n","            x = next(iterator, None)\n","    def __next__(self) -> tuple:\n","        \"\"\"Analogous behaviour to the native Python next() but calling the\n","        .pop() of the data attribute.\n","        \"\"\"\n","        try:\n","            return self.data.pop()\n","        except IndexError:  # Caught when the data set runs out of elements\n","            self._init_data(\n","                iter(torch.utils.data.DataLoader(self.X, batch_size=self.batch_size, shuffle=True))\n","            )\n","            return self.data.pop()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We call an instance of the above in an electron<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def get_training_cycler(Xtr: torch.Tensor, batch_size: int, seed: int = GLOBAL_SEED) -> DataGetter:\n","    \"\"\"Get an instance of the DataGetter class defined above, which behaves analogously to\n","    next(iterator) but is pickleable.\n","    \"\"\"\n","    return DataGetter(Xtr, batch_size, seed)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We now have the means to create synthetic data and cycle through a<br>\n","training set. Next, we need to build our loss function<br>\n",":math:`\\mathcal{L}(\\boldsymbol{\\phi})` from electrons with the help of<br>\n","``PennyLane``.<br>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Building the loss function<br>\n","--------------------------<br>\n","<br>\n","Core to building the loss function is the quantum circuit implementing<br>\n",":math:`V_t(\\boldsymbol{\\alpha}, \\boldsymbol{\\gamma}) := W^{\\dagger}(\\boldsymbol{\\alpha})D(\\boldsymbol{\\gamma}, t)W(\\boldsymbol{\\alpha})`.<br>\n","While there are existing templates in ``PennyLane`` for implementing<br>\n",":math:`W(\\boldsymbol{\\alpha})`, we use a custom circuit to implement<br>\n",":math:`D(\\boldsymbol{\\gamma}, t)`. Following the approach taken in<br>\n","[#Welch2014]_ (also explained in [#Baker2022]_ and the<br>\n","appendix of [#Cîrstoiu2020]_), we create the electron:<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pennylane as qml\n","from itertools import combinations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def D(gamma: torch.Tensor, n_qubits: int, k: int = None, get_probs: bool = False) -> None:\n","    \"\"\"Generates an n_qubit quantum circuit according to a k-local Walsh operator\n","    expansion. Here, k-local means that 1 <= k <= n of the n qubits can interact.\n","    See <https://doi.org/10.1088/1367-2630/16/3/033040> for more\n","    details. Optionally return probabilities of bit strings.\n","    \"\"\"\n","    if k is None:\n","        k = n_qubits\n","    cnt = 0\n","    for i in range(1, k + 1):\n","        for comb in combinations(range(n_qubits), i):\n","            if len(comb) == 1:\n","                qml.RZ(gamma[cnt], wires=[comb[0]])\n","                cnt += 1\n","            elif len(comb) > 1:\n","                cnots = [comb[i : i + 2] for i in range(len(comb) - 1)]\n","                for j in cnots:\n","                    qml.CNOT(wires=j)\n","                qml.RZ(gamma[cnt], wires=[comb[-1]])\n","                cnt += 1\n","                for j in cnots[::-1]:\n","                    qml.CNOT(wires=j)\n","    if get_probs:\n","        return qml.probs(wires=range(n_qubits))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["While the above may seem a little complicated, since we only use a single<br>\n","qubit in this tutorial, the resulting circuit is merely a single :math:`R_z(\\theta)` gate."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_qubits = 1\n","dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\n","D_one_qubit = qml.qnode(dev)(D)\n","_ = qml.draw_mpl(D_one_qubit, decimals=2)(torch.tensor([1, 0]), 1, 1, True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["You may find the general function for :math:`D`` useful in case you want to experiment<br>\n","with more qubits and your own (possibly multi-dimensional) data after<br>\n","this tutorial.<br>\n","<br>\n","Next, we define a circuit to calculate the probability of certain bit strings being measured in the<br>\n","computational basis. In our simple example, we work only with one qubit<br>\n","and use the ``default.qubit`` local quantum circuit simulator.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n","def get_probs(\n","    xt: torch.Tensor,\n","    t: float,\n","    alpha: torch.Tensor,\n","    gamma: torch.Tensor,\n","    k: int,\n","    U: callable,\n","    W: callable,\n","    D: callable,\n","    n_qubits: int,\n",") -> torch.Tensor:\n","    \"\"\"Measure the probabilities for measuring each bitstring after applying a\n","    circuit of the form W†DWU to the |0⟩^(⊗n) state. This\n","    function is defined for individual sequence elements xt.\n","    \"\"\"\n","    U(xt, wires=range(n_qubits))\n","    W(alpha, wires=range(n_qubits))\n","    D(gamma * t, n_qubits, k)\n","    qml.adjoint(W)(alpha, wires=range(n_qubits))\n","    return qml.probs(range(n_qubits))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["To take the projector<br>\n",":math:`|0\\rangle^{\\otimes n} \\langle 0 |^{\\otimes n}`, we consider only<br>\n","the probability of measuring the bit string of all zeroes, which is the<br>\n","0th element of the probabilities (bit strings are returned in<br>\n","lexicographic order).<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def get_callable_projector_func(\n","    k: int, U: callable, W: callable, D: callable, n_qubits: int, probs_func: callable\n",") -> callable:\n","    \"\"\"Using get_probs() above, take only the probability of measuring the\n","    bitstring of all zeroes (i.e, take the projector\n","    |0⟩^(⊗n)⟨0|^(⊗n)) on the time devolved state.\n","    \"\"\"\n","    callable_proj = lambda xt, t, alpha, gamma: probs_func(\n","        xt, t, alpha, gamma, k, U, W, D, n_qubits\n","    )[0]\n","    return callable_proj"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We now have the necessary ingredients to build<br>\n",":math:`F(\\boldsymbol{\\phi}, x_t)`.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def F(\n","    callable_proj: callable,\n","    xt: torch.Tensor,\n","    t: float,\n","    alpha: torch.Tensor,\n","    mu: torch.Tensor,\n","    sigma: torch.Tensor,\n","    gamma_length: int,\n","    n_samples: int,\n",") -> torch.Tensor:\n","    \"\"\"Take the classical expecation value of of the projector on zero sampling\n","    the parameters of D from normal distributions. The expecation value is estimated\n","    with an average over n_samples.\n","    \"\"\"\n","    # length of gamma should not exceed 2^n - 1\n","    gammas = sigma.abs() * torch.randn((n_samples, gamma_length)) + mu\n","    expectation = torch.empty(n_samples)\n","    for i, gamma in enumerate(gammas):\n","        expectation[i] = callable_proj(xt, t, alpha, gamma)\n","    return expectation.mean()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We now return to the matter of the penalty function :math:`P_{\\tau}`.<br>\n","We choose<br>\n","<br>\n",".. math::<br>\n","<br>\n","<br>\n","   P_{\\tau}(\\sigma) := \\frac{1}{\\pi} \\arctan(2 \\pi \\tau |\\sigma|).<br>\n","<br>\n","As an electron, we have"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def callable_arctan_penalty(tau: float) -> callable:\n","    \"\"\"Create a callable arctan function with a single hyperparameter\n","    tau to penalize large entries of sigma.\n","    \"\"\"\n","    prefac = 1 / (torch.pi)\n","    callable_pen = lambda sigma: prefac * torch.arctan(2 * torch.pi * tau * sigma.abs()).mean()\n","    return callable_pen"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The above is a sigmoidal function chosen because it comes with the useful property of being bounded.<br>\n","The prefactor of :math:`1/\\pi` is chosen such that the final loss<br>\n",":math:`\\mathcal{L}(\\boldsymbol{\\phi})` is defined in the range (0, 1),<br>\n","as defined in the below electron.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def get_loss(\n","    callable_proj: callable,\n","    batch: torch.Tensor,\n","    alpha: torch.Tensor,\n","    mu: torch.Tensor,\n","    sigma: torch.Tensor,\n","    gamma_length: int,\n","    n_samples: int,\n","    callable_penalty: callable,\n",") -> torch.Tensor:\n","    \"\"\"Evaluate the loss function ℒ, defined in the background section\n","    for a certain set of parameters.\n","    \"\"\"\n","    X_batch, T_batch = batch\n","    loss = torch.empty(X_batch.size()[0])\n","    for i in range(X_batch.size()[0]):\n","        # unsqueeze required for tensor to have the correct dimension for PennyLane templates\n","        loss[i] = (\n","            1\n","            - F(\n","                callable_proj,\n","                X_batch[i].unsqueeze(0),\n","                T_batch[i].unsqueeze(0),\n","                alpha,\n","                mu,\n","                sigma,\n","                gamma_length,\n","                n_samples,\n","            )\n","        ).square()\n","    return 0.5 * loss.mean() + callable_penalty(sigma)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Training the normal model<br>\n","-------------------------<br>\n","<br>\n","Now equipped with a loss function, we need to minimize it with a<br>\n","classical optimization routine. To start this optimization, however, we<br>\n","need some initial parameters. We can generate them with the below<br>\n","electron.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def get_initial_parameters(\n","    W: callable, W_layers: int, n_qubits: int, seed: int = GLOBAL_SEED\n",") -> dict:\n","    \"\"\"Randomly generate initial parameters. We need initial parameters for the\n","    variational circuit ansatz implementing W(alpha) and the standard deviation\n","    and mean (sigma and mu) for the normal distribution we sample gamma from.\n","    \"\"\"\n","    torch.manual_seed(seed)\n","    init_alpha = torch.rand(W.shape(W_layers, n_qubits))\n","    init_mu = torch.rand(1)\n","    # Best to start sigma small and expand if needed\n","    init_sigma = torch.rand(1)\n","    init_params = {\n","        \"alpha\": (2 * torch.pi * init_alpha).clone().detach().requires_grad_(True),\n","        \"mu\": (2 * torch.pi * init_mu).clone().detach().requires_grad_(True),\n","        \"sigma\": (0.1 * init_sigma + 0.05).clone().detach().requires_grad_(True),\n","    }\n","    return init_params"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Using the ``PyTorch`` interface to ``PennyLane``, we define our final<br>\n","electron before running the training workflow.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def train_model_gradients(\n","    lr: float,\n","    init_params: dict,\n","    pytorch_optimizer: callable,\n","    cycler: DataGetter,\n","    n_samples: int,\n","    callable_penalty: callable,\n","    batch_iterations: int,\n","    callable_proj: callable,\n","    gamma_length: int,\n","    seed=GLOBAL_SEED,\n","    print_intermediate=False,\n",") -> dict:\n","    \"\"\"Train the QVR model (minimize the loss function) with respect to the\n","    variational parameters using gradient-based training. You need to pass a\n","    PyTorch optimizer and a learning rate (lr).\n","    \"\"\"\n","    torch.manual_seed(seed)\n","    opt = pytorch_optimizer(init_params.values(), lr=lr)\n","    alpha = init_params[\"alpha\"]\n","    mu = init_params[\"mu\"]\n","    sigma = init_params[\"sigma\"]\n","    def closure():\n","        opt.zero_grad()\n","        loss = get_loss(\n","            callable_proj, next(cycler), alpha, mu, sigma, gamma_length, n_samples, callable_penalty\n","        )\n","        loss.backward()\n","        return loss\n","    loss_history = []\n","    for i in range(batch_iterations):\n","        loss = opt.step(closure)\n","        loss_history.append(loss.item())\n","        if batch_iterations % 10 == 0 and print_intermediate:\n","            print(f\"Iteration number {i}\\n Current loss {loss.item()}\\n\")\n","    results_dict = {\n","        \"opt_params\": {\n","            \"alpha\": opt.param_groups[0][\"params\"][0],\n","            \"mu\": opt.param_groups[0][\"params\"][1],\n","            \"sigma\": opt.param_groups[0][\"params\"][2],\n","        },\n","        \"loss_history\": loss_history,\n","    }\n","    return results_dict"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, enter our first ``@ct.lattice``. This combines the above electrons,<br>\n","eventually returning the optimal parameters<br>\n",":math:`\\boldsymbol{\\phi}^{\\star}` and the loss with batch iterations.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.lattice\n","def training_workflow(\n","    U: callable,\n","    W: callable,\n","    D: callable,\n","    n_qubits: int,\n","    k: int,\n","    probs_func: callable,\n","    W_layers: int,\n","    gamma_length: int,\n","    n_samples: int,\n","    p: int,\n","    num_series: int,\n","    noise_amp: float,\n","    t_init: float,\n","    t_end: float,\n","    batch_size: int,\n","    tau: float,\n","    pytorch_optimizer: callable,\n","    lr: float,\n","    batch_iterations: int,\n","):\n","    \"\"\"\n","    Combine all of the previously defined electrons to do an entire training workflow,\n","    including (1) generating synthetic data, (2) packaging it into training cyclers\n","    (3) preparing the quantum functions and (4) optimizing the loss function with\n","    gradient based optimization. You can find definitions for all of the arguments\n","    by looking at the electrons and text cells above.\n","    \"\"\"\n","    X, T = generate_normal_time_series_set(p, num_series, noise_amp, t_init, t_end)\n","    Xtr = make_atomized_training_set(X, T)\n","    cycler = get_training_cycler(Xtr, batch_size)\n","    init_params = get_initial_parameters(W, W_layers, n_qubits)\n","    callable_penalty = callable_arctan_penalty(tau)\n","    callable_proj = get_callable_projector_func(k, U, W, D, n_qubits, probs_func)\n","    results_dict = train_model_gradients(\n","        lr,\n","        init_params,\n","        pytorch_optimizer,\n","        cycler,\n","        n_samples,\n","        callable_penalty,\n","        batch_iterations,\n","        callable_proj,\n","        gamma_length,\n","        print_intermediate=False,\n","    )\n","    return results_dict"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before running this workflow, we define all of the input parameters.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["general_options = {\n","    \"U\": qml.AngleEmbedding,\n","    \"W\": qml.StronglyEntanglingLayers,\n","    \"D\": D,\n","    \"n_qubits\": 1,\n","    \"probs_func\": get_probs,\n","    \"gamma_length\": 1,\n","    \"n_samples\": 10,\n","    \"p\": 25,\n","    \"num_series\": 25,\n","    \"noise_amp\": 0.1,\n","    \"t_init\": 0.1,\n","    \"t_end\": 2 * torch.pi,\n","    \"k\": 1,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_options = {\n","    \"batch_size\": 10,\n","    \"tau\": 5,\n","    \"pytorch_optimizer\": torch.optim.Adam,\n","    \"lr\": 0.01,\n","    \"batch_iterations\": 100,\n","    \"W_layers\": 2,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_options.update(general_options)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can now dispatch the lattice to the Covalent server.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tr_dispatch_id = ct.dispatch(training_workflow)(**training_options)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["If you are running the notebook version of this tutorial, if you<br>\n","navigate to http://localhost:48008/ you can view the workflow on the<br>\n","Covalent GUI. It will look like the screenshot below, showing nicely how all of the<br>\n","electrons defined above interact with each other in the workflow. You can<br>\n","also track the progress of the calculation here.<br>\n","<br>\n",".. figure:: ../demonstrations/univariate_qvr/covalent_tutorial_screenshot.png<br>\n","   :width: 85%<br>\n","   :align: center<br>\n","   :alt: Training workflow screenshot in Covalent<br>\n","<br>\n","<br>\n","   A screenshot of the Covalent GUI for the training workflow.<br>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We now pull the results back from the Covalent server:<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ct_tr_results = ct.get_result(dispatch_id=tr_dispatch_id, wait=True)\n","results_dict = ct_tr_results.result"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["and take a look at the training loss history:<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure()\n","plt.plot(results_dict[\"loss_history\"], \".-\")\n","plt.ylabel(\"Loss [$\\mathcal{L}$]\")\n","plt.xlabel(\"Batch iterations\")\n","plt.title(\"Loss function versus batch iterations in training\")\n","plt.grid()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Tuning the threshold :math:`\\zeta`<br>\n","----------------------------------<br>\n","<br>\n","When we have access to labelled anomalous series (as we do in our toy<br>\n","problem here, often not the case in reality), we can tune the threshold<br>\n",":math:`\\zeta` to maximize some success metric. We choose to maximize the<br>\n","accuracy score as defined using the three electrons below.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def get_preds_given_threshold(zeta: float, scores: torch.Tensor) -> torch.Tensor:\n","    \"\"\"For a given threshold, get the predicted labels (1 or -1), given the anomaly scores.\"\"\"\n","    return torch.tensor([-1 if score > zeta else 1 for score in scores])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def get_truth_labels(\n","    normal_series_set: torch.Tensor, anomalous_series_set: torch.Tensor\n",") -> torch.Tensor:\n","    \"\"\"Get a 1D tensor containing the truth values (1 or -1) for a given set of\n","    time series.\n","    \"\"\"\n","    norm = torch.ones(normal_series_set.size()[0])\n","    anom = -torch.ones(anomalous_series_set.size()[0])\n","    return torch.cat([norm, anom])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def get_accuracy_score(pred: torch.Tensor, truth: torch.Tensor) -> torch.Tensor:\n","    \"\"\"Given the predictions and truth values, return a number between 0 and 1\n","    indicating the accuracy of predictions.\n","    \"\"\"\n","    return torch.sum(pred == truth) / truth.size()[0]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Then, knowing the anomaly scores :math:`a_X(y)` for a validation data<br>\n","set, we can scan through various values of :math:`\\zeta` on a fine 1D grid and calcuate<br>\n","the accuracy score. Our goal is to pick the :math:`\\zeta` with the<br>\n","largest accuracy score.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def threshold_scan_acc_score(\n","    scores: torch.Tensor, truth_labels: torch.Tensor, zeta_min: float, zeta_max: float, steps: int\n",") -> torch.Tensor:\n","    \"\"\"Given the anomaly scores and truth values,\n","    scan over a range of thresholds = [zeta_min, zeta_max] with a\n","    fixed number of steps, calculating the accuracy score at each point.\n","    \"\"\"\n","    accs = torch.empty(steps)\n","    for i, zeta in enumerate(torch.linspace(zeta_min, zeta_max, steps)):\n","        preds = get_preds_given_threshold(zeta, scores)\n","        accs[i] = get_accuracy_score(preds, truth_labels)\n","    return accs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def get_anomaly_score(\n","    callable_proj: callable,\n","    y: torch.Tensor,\n","    T: torch.Tensor,\n","    alpha_star: torch.Tensor,\n","    mu_star: torch.Tensor,\n","    sigma_star: torch.Tensor,\n","    gamma_length: int,\n","    n_samples: int,\n","    get_time_resolved: bool = False,\n","):\n","    \"\"\"Get the anomaly score for an input time series y. We need to pass the\n","    optimal parameters (arguments with suffix _star). Optionally return the\n","    time-resolved score (the anomaly score contribution at a given t).\n","    \"\"\"\n","    scores = torch.empty(T.size()[0])\n","    for i in range(T.size()[0]):\n","        scores[i] = (\n","            1\n","            - F(\n","                callable_proj,\n","                y[i].unsqueeze(0),\n","                T[i].unsqueeze(0),\n","                alpha_star,\n","                mu_star,\n","                sigma_star,\n","                gamma_length,\n","                n_samples,\n","            )\n","        ).square()\n","    if get_time_resolved:\n","        return scores, scores.mean()\n","    else:\n","        return scores.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.electron\n","def get_norm_and_anom_scores(\n","    X_norm: torch.Tensor,\n","    X_anom: torch.Tensor,\n","    T: torch.Tensor,\n","    callable_proj: callable,\n","    model_params: dict,\n","    gamma_length: int,\n","    n_samples: int,\n",") -> torch.Tensor:\n","    \"\"\"Get the anomaly scores assigned to input normal and anomalous time series instances.\n","    model_params is a dictionary containing the optimal model parameters.\n","    \"\"\"\n","    alpha = model_params[\"alpha\"]\n","    mu = model_params[\"mu\"]\n","    sigma = model_params[\"sigma\"]\n","    norm_scores = torch.tensor(\n","        [\n","            get_anomaly_score(callable_proj, xt, T, alpha, mu, sigma, gamma_length, n_samples)\n","            for xt in X_norm\n","        ]\n","    )\n","    anom_scores = torch.tensor(\n","        [\n","            get_anomaly_score(callable_proj, xt, T, alpha, mu, sigma, gamma_length, n_samples)\n","            for xt in X_anom\n","        ]\n","    )\n","    return torch.cat([norm_scores, anom_scores])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We now create our second ``@ct.lattice``. We are to test the optimal<br>\n","model against two random models. If our model is trainable, we should<br>\n","see that the trained model is better.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.lattice\n","def threshold_tuning_workflow(\n","    opt_params: dict,\n","    gamma_length: int,\n","    n_samples: int,\n","    probs_func: callable,\n","    zeta_min: float,\n","    zeta_max: float,\n","    steps: int,\n","    p: int,\n","    num_series: int,\n","    noise_amp: float,\n","    spike_amp: float,\n","    max_duration: int,\n","    t_init: float,\n","    t_end: float,\n","    k: int,\n","    U: callable,\n","    W: callable,\n","    D: callable,\n","    n_qubits: int,\n","    random_model_seeds: torch.Tensor,\n","    W_layers: int,\n",") -> tuple:\n","    \"\"\"A workflow for tuning the threshold value zeta, in order to maximize the accuracy score\n","    for a validation data set. Results are tested against random models at their optimal zetas.\n","    \"\"\"\n","    # Generate datasets\n","    X_val_norm, T = generate_normal_time_series_set(p, num_series, noise_amp, t_init, t_end)\n","    X_val_anom, T = generate_anomalous_time_series_set(\n","        p, num_series, noise_amp, spike_amp, max_duration, t_init, t_end\n","    )\n","    truth_labels = get_truth_labels(X_val_norm, X_val_anom)\n","\n","    # Initialize quantum functions\n","    callable_proj = get_callable_projector_func(k, U, W, D, n_qubits, probs_func)\n","    accs_list = []\n","    scores_list = []\n","    # Evaluate optimal model\n","    scores = get_norm_and_anom_scores(\n","        X_val_norm, X_val_anom, T, callable_proj, opt_params, gamma_length, n_samples\n","    )\n","    accs_opt = threshold_scan_acc_score(scores, truth_labels, zeta_min, zeta_max, steps)\n","    accs_list.append(accs_opt)\n","    scores_list.append(scores)\n","\n","    # Evaluate random models\n","    for seed in random_model_seeds:\n","        rand_params = get_initial_parameters(W, W_layers, n_qubits, seed)\n","        scores = get_norm_and_anom_scores(\n","            X_val_norm, X_val_anom, T, callable_proj, rand_params, gamma_length, n_samples\n","        )\n","        accs_list.append(threshold_scan_acc_score(scores, truth_labels, zeta_min, zeta_max, steps))\n","        scores_list.append(scores)\n","    return accs_list, scores_list"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We now set the input parameters.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["threshold_tuning_options = {\n","    \"spike_amp\": 0.4,\n","    \"max_duration\": 5,\n","    \"zeta_min\": 0,\n","    \"zeta_max\": 1,\n","    \"steps\": 100000,\n","    \"random_model_seeds\": [0, 1],\n","    \"W_layers\": 2,\n","    \"opt_params\": results_dict[\"opt_params\"],\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["threshold_tuning_options.update(general_options)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As before, we dispatch the lattice to the ``Covalent`` server.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_dispatch_id = ct.dispatch(threshold_tuning_workflow)(**threshold_tuning_options)\n","ct_val_results = ct.get_result(dispatch_id=val_dispatch_id, wait=True)\n","accs_list, scores_list = ct_val_results.result"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, we can plot the results:<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["zeta_xlims = [(0, 0.001), (0.25, 0.38), (0.25, 0.38)]\n","titles = [\"Trained model\", \"Random model 1\", \"Random model 2\"]\n","zetas = torch.linspace(\n","    threshold_tuning_options[\"zeta_min\"],\n","    threshold_tuning_options[\"zeta_max\"],\n","    threshold_tuning_options[\"steps\"],\n",")\n","fig, axs = plt.subplots(ncols=3, nrows=2, sharey=\"row\")\n","for i in range(3):\n","    axs[0, i].plot(zetas, accs_list[i])\n","    axs[0, i].set_xlim(zeta_xlims[i])\n","    axs[0, i].set_xlabel(\"Threshold [$\\zeta$]\")\n","    axs[0, i].set_title(titles[i])\n","    axs[1, i].boxplot(\n","        [\n","            scores_list[i][0 : general_options[\"num_series\"]],\n","            scores_list[i][general_options[\"num_series\"] : -1],\n","        ],\n","        labels=[\"Normal\", \"Anomalous\"],\n","    )\n","    axs[1, i].set_yscale(\"log\")\n","    axs[1, i].axhline(\n","        zetas[torch.argmax(accs_list[i])], color=\"k\", linestyle=\":\", label=\"Optimal $\\zeta$\"\n","    )\n","    axs[1, i].legend()\n","axs[0, 0].set_ylabel(\"Accuracy score\")\n","axs[1, 0].set_ylabel(\"Anomaly score [$a_X(y)$]\")\n","fig.tight_layout()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Parsing the above, we can see that the optimal model achieves high<br>\n","accuracy when the threshold is tuned using the validation data.<br>\n","On the other hand, the random models return mostly random results<br>\n","(sometimes even worse than random guesses), regardless of how we set the<br>\n","threshold.<br>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Testing the model<br>\n","-----------------<br>\n","<br>\n","Now with optimal thresholds for our optimized and random models, we can perform testing.<br>\n","We already have all of the electrons to do this, so we create<br>\n","the ``@ct.lattice``<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@ct.lattice\n","def testing_workflow(\n","    opt_params: dict,\n","    gamma_length: int,\n","    n_samples: int,\n","    probs_func: callable,\n","    best_zetas: list,\n","    p: int,\n","    num_series: int,\n","    noise_amp: float,\n","    spike_amp: float,\n","    max_duration: int,\n","    t_init: float,\n","    t_end: float,\n","    k: int,\n","    U: callable,\n","    W: callable,\n","    D: callable,\n","    n_qubits: int,\n","    random_model_seeds: torch.Tensor,\n","    W_layers: int,\n",") -> list:\n","    \"\"\"A workflow for calculating anomaly scores for a set of testing time series\n","    given an optimal model and set of random models. We use the optimal zetas found in threshold tuning.\n","    \"\"\"\n","    # Generate time series\n","    X_val_norm, T = generate_normal_time_series_set(p, num_series, noise_amp, t_init, t_end)\n","    X_val_anom, T = generate_anomalous_time_series_set(\n","        p, num_series, noise_amp, spike_amp, max_duration, t_init, t_end\n","    )\n","    truth_labels = get_truth_labels(X_val_norm, X_val_anom)\n","\n","    # Prepare quantum functions\n","    callable_proj = get_callable_projector_func(k, U, W, D, n_qubits, probs_func)\n","    accs_list = []\n","    # Evaluate optimal model\n","    scores = get_norm_and_anom_scores(\n","        X_val_norm, X_val_anom, T, callable_proj, opt_params, gamma_length, n_samples\n","    )\n","    preds = get_preds_given_threshold(best_zetas[0], scores)\n","    accs_list.append(get_accuracy_score(preds, truth_labels))\n","    # Evaluate random models\n","    for zeta, seed in zip(best_zetas[1:], random_model_seeds):\n","        rand_params = get_initial_parameters(W, W_layers, n_qubits, seed)\n","        scores = get_norm_and_anom_scores(\n","            X_val_norm, X_val_anom, T, callable_proj, rand_params, gamma_length, n_samples\n","        )\n","        preds = get_preds_given_threshold(zeta, scores)\n","        accs_list.append(get_accuracy_score(preds, truth_labels))\n","    return accs_list"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We dispatch it to the Covalent server with the appropriate parameters.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["testing_options = {\n","    \"spike_amp\": 0.4,\n","    \"max_duration\": 5,\n","    \"best_zetas\": [zetas[torch.argmax(accs)] for accs in accs_list],\n","    \"random_model_seeds\": [0, 1],\n","    \"W_layers\": 2,\n","    \"opt_params\": results_dict[\"opt_params\"],\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["testing_options.update(general_options)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_dispatch_id = ct.dispatch(testing_workflow)(**testing_options)\n","ct_test_results = ct.get_result(dispatch_id=test_dispatch_id, wait=True)\n","accs_list = ct_test_results.result"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Finally, we plot the results below.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure()\n","plt.bar([1, 2, 3], accs_list)\n","plt.axhline(0.5, color=\"k\", linestyle=\":\", label=\"Random accuracy\")\n","plt.xticks([1, 2, 3], [\"Trained model\", \"Random model 1\", \"Random model 2\"])\n","plt.ylabel(\"Accuracy score\")\n","plt.title(\"Accuracy scores for trained and random models\")\n","leg = plt.legend()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As can be seen, once more, the trained model is far more accurate than<br>\n","the random models. Awesome! Now that we're done with the calculations in<br>\n","this tutorial, we just need to remember to shut down the Covalent server<br>\n"]}],"metadata":{"kernelspec":{"display_name":"pennylane-nb","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"fd869327155fe353a03632081ba966768ca3fb2acd5f6c90184782c22042853d"}}},"nbformat":4,"nbformat_minor":2}
