{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc9463b",
   "metadata": {},
   "source": [
    "## Classifying discrete spacetimes by dimension\n",
    "\n",
    "Note: Breaks on M1 Macs because of `tensorflow` causing kernel panic.\n",
    "\n",
    "[The causal set approach to quantum gravity](https://link.springer.com/article/10.1007/s41114-019-0023-1) postulates spacetime is fundamentally comprised of a set of \"spacetime atoms\", also called elements, together with a set of pairwise causal relations. This approach is deeply rooted in discrete geometry and topology, as well as order theory, and it appeals to practitioners who are minimalists in terms of underlying assumptions of fundamental physics.  While there are many interesting open problems connecting the discrete, quantum world to the continuous, classical realm we are familiar with, in this tutorial we focus on a relatively simple question: is it possible, using machine learning methods, to infer the embedding dimension of a finite causal set spacetime?  If so, what are some of the challenges in accurately measuring such an observable, particularly for small spacetimes on the order of 100 Planck volumes ($10^{-103}m^3$)?\n",
    "\n",
    "We start by importing Covalent, as well as some other relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019ba64-1391-484b-affb-223591eb1831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import covalent as ct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f2126b-7cd1-4c85-b86e-47f6e9f84849",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "In the first part we generate training data for the ML task. Each sample is a sequence of $N$ integers which characterize a single discrete flat spacetime. Samples are generated for either 2D or 3D spacetimes using [Poisson sampling](https://en.wikipedia.org/wiki/Poisson_point_process) in a unit-height Alexandroff interval.  Each coordinate represents an atom of spacetime, also called an element.  Once elements are sampled, we calculate the adjacency matrix of causal relations. A relation \"$\\prec$\" is said to exist between a pair of spacetime elements $(x,y)$ iff they are timelike separated, i.e., if a signal can travel between the points at less than the speed of light.  At this point each spacetime is characterized by $O(N^2)$ degrees of freedom. However, the topology and geometry can be characterized by $O(N)$ degrees of freedom by calculating the size distribution of the \"order intervals\". That is, for each pair of elements $(x,y)$, with $x\\prec y$ count the number of elements $z$ which satisfy $x\\prec z\\prec y$. The distribution of these sizes describes the distribution of neighborhood sizes in Lorentzian spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352b209-715b-45e9-8915-0ab9e295fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single spacetime\n",
    "@ct.electron\n",
    "def generate_flat_spacetime(\n",
    "    num_elements: int = 100, \n",
    "    dim: int = 2\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    if dim == 2:\n",
    "        # Sample light cone coordinates in 2D\n",
    "        u = np.random.random(num_elements)\n",
    "        v = np.random.random(num_elements)\n",
    "        \n",
    "        # Rotate to Lorentzian coordinates\n",
    "        t = (u + v) / np.sqrt(2)\n",
    "        x = (u - v) / np.sqrt(2)\n",
    "        \n",
    "        # Format the coordinates in a dataframe\n",
    "        df = pd.DataFrame(zip(x,t), columns=['x','t'])\n",
    "        df.sort_values('t', inplace=True, ignore_index=True)\n",
    "        return df\n",
    "    elif dim == 3:\n",
    "        # Sample light cone coordinates in 3D\n",
    "        u = np.random.random(num_elements) ** (1. / 3)\n",
    "        v = u - np.sqrt(u * u * (1 - np.random.random(num_elements)))\n",
    "        theta = 2 * np.pi * np.random.random(num_elements)\n",
    "        \n",
    "        # Coordinate transformation\n",
    "        t = (u + v) / np.sqrt(2)\n",
    "        x = ((u - v) / np.sqrt(2)) * np.cos(theta)\n",
    "        y = ((u - v) / np.sqrt(2)) * np.sin(theta)\n",
    "        \n",
    "        # Format the coordinates in a dataframe\n",
    "        df = pd.DataFrame(zip(x,y,t), columns=['x','y','t'])\n",
    "        df.sort_values('t', inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(f\"Dimension {dim} is not supported!\")\n",
    "\n",
    "# Visualize a single spacetime\n",
    "def visualize_spacetime(coords: pd.DataFrame) -> None:\n",
    "    \n",
    "    dim = len(coords.columns)\n",
    "    \n",
    "    if dim == 2:\n",
    "        ax = coords.plot.scatter(x='x', y='t')\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_facecolor('white')\n",
    "    elif dim == 3:\n",
    "        ax = coords.plot.scatter(x='x', y='t')\n",
    "        ax.set_aspect('equal')\n",
    "        ay = coords.plot.scatter(x='x', y='y')\n",
    "        ay.set_aspect('equal')\n",
    "    else:\n",
    "        raise Exception(f\"Dimension {dim} is not supported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e49e22",
   "metadata": {},
   "source": [
    "Let's pause here to visualize a single discrete spacetime.  We execute the function `generate_flat_spacetime` directly by wrapping it as a lattice and dispatching it. In two dimensions, for instance, we see the region is a diamond shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66501c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatch_id = ct.dispatch(ct.lattice(generate_flat_spacetime))(num_elements=1000, dim=2)\n",
    "coordinates = ct.get_result(dispatch_id, wait=True).result\n",
    "visualize_spacetime(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73db9ca",
   "metadata": {},
   "source": [
    "Now we generate an ensemble of such spacetimes and calculate the causal relations for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac147ba7-c256-4753-872e-90af7eb17a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an ensemble of spacetimes\n",
    "@ct.electron\n",
    "def generate_ensemble(\n",
    "    ensemble_size: int = 1000, \n",
    "    num_elements: int = 100, \n",
    "    dim: int = 2\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    return pd.concat([\n",
    "        generate_flat_spacetime(num_elements=num_elements, dim=dim) \\\n",
    "        for x in range(ensemble_size)\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720e329-24da-4474-95dd-487076783f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify causal relations between elements\n",
    "@ct.electron\n",
    "def is_related(\n",
    "    c1: pd.DataFrame, \n",
    "    c2: pd.DataFrame, \n",
    "    dim: int\n",
    ") -> bool:\n",
    "    \n",
    "    related = False\n",
    "    \n",
    "    if dim == 2:\n",
    "        dx = abs(c1['x'] - c2['x'])\n",
    "        dt = abs(c1['t'] - c2['t'])\n",
    "\n",
    "        related = dx < dt\n",
    "    elif dim == 3:\n",
    "        dx = np.sqrt((c1['x'] - c2['x']) ** 2 + (c1['y'] - c2['y']) ** 2)\n",
    "        dt = abs(c1['t'] - c2['t'])\n",
    "        \n",
    "        related = dx < dt\n",
    "    else:\n",
    "        raise Exception(f\"Dimension {dim} is not supported!\")\n",
    "    \n",
    "    return related\n",
    "\n",
    "# Generate adjacency matrices\n",
    "@ct.electron\n",
    "def generate_adj_matrices(\n",
    "    ensemble: pd.DataFrame,\n",
    "    num_elements: int, dim: int\n",
    ") -> np.ndarray:\n",
    "    \n",
    "    num_spacetimes = int(ensemble.shape[0] / num_elements)\n",
    "    adjmat = np.zeros((num_spacetimes,num_elements,num_elements), dtype=bool)\n",
    "    \n",
    "    for st in range(num_spacetimes):\n",
    "        coords = ensemble[st*num_elements:(st+1)*num_elements]\n",
    "        for i, c in coords.iterrows():\n",
    "            for j, d in coords.iterrows():\n",
    "                adjmat[st,i%num_elements,j%num_elements] = is_related(c1=c, c2=d, dim=dim)\n",
    "    \n",
    "    return adjmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256f7d2-42e1-4547-8b0a-b4fe5727c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate order intervals\n",
    "# Reduces d.o.f. to O(num_elements)\n",
    "@ct.electron\n",
    "def calculate_intervals(\n",
    "    adjmat: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    num_spacetimes = len(adjmat)\n",
    "    num_elements = len(adjmat[0,0])\n",
    "    \n",
    "    intervals = np.zeros((num_spacetimes, num_elements), dtype=float)\n",
    "    for st, A in enumerate(adjmat):\n",
    "        intervals[st,0] = num_elements\n",
    "        for i in range(num_elements):\n",
    "            for j in range(i + 1, num_elements):\n",
    "                count = 0\n",
    "                for k in range(i + 1, j):\n",
    "                    count += int(A[i,k] and A[k,j])\n",
    "                intervals[st,count+1] += 1\n",
    "    \n",
    "    return pd.DataFrame(intervals)\n",
    "\n",
    "# Calculate all intervals across dimensions\n",
    "@ct.electron\n",
    "def calculate_all_intervals(\n",
    "    relations: List[np.ndarray]\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    return pd.concat([\n",
    "        calculate_intervals(adjmat=r) for r in relations\n",
    "    ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209d588-a073-42ce-aef0-7e3b4401e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct training classes\n",
    "@ct.electron\n",
    "def construct_classes(\n",
    "    nsamples: int, \n",
    "    dimensions: List[int]\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    classes = pd.DataFrame()\n",
    "    \n",
    "    for idx, dim in enumerate(dimensions):\n",
    "        classes[f\"{dim}D\"] = pd.concat([\n",
    "            pd.Series([0 for x in range(nsamples*idx)], dtype=float),\n",
    "            pd.Series([1 for x in range(nsamples)], dtype=float),\n",
    "            pd.Series([0 for x in range(\n",
    "                nsamples*(idx+1), \n",
    "                nsamples*len(dimensions)\n",
    "            )], dtype=float)\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b34868",
   "metadata": {},
   "source": [
    "In order to generate the training data, we now combine the above functions into a lattice. There are three computationally heavy tasks which we serialize: the $O(N)$ coordinate sampling, the $O(N^2)$ relations calculation, and the $O(N^3)$ order interval calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b46ed5-3e71-4d33-b69c-5c11aca1688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lattice to generate the data\n",
    "@ct.lattice\n",
    "def generate_training_data(\n",
    "    nsamples, \n",
    "    num_elements, \n",
    "    dimensions\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \n",
    "    # Generate the coordinate ensemble\n",
    "    spacetimes = [ \n",
    "        generate_ensemble(\n",
    "            ensemble_size=nsamples, \n",
    "            num_elements=num_elements, \n",
    "            dim=d\n",
    "        ) \n",
    "        for d in dimensions\n",
    "    ]\n",
    "    \n",
    "    # Calculate causal relations\n",
    "    relations = [\n",
    "        generate_adj_matrices(\n",
    "            ensemble=st,\n",
    "            num_elements=num_elements,\n",
    "            dim=d\n",
    "        ) for st, d in zip(spacetimes, dimensions)\n",
    "    ]\n",
    "    \n",
    "    # Calculate order intervals (learning model inputs)\n",
    "    predictors = calculate_all_intervals(relations=relations)\n",
    "    \n",
    "    # Construct training classes (learning model outputs)\n",
    "    classes = construct_classes(nsamples=nsamples, dimensions=dimensions)\n",
    "    \n",
    "    return predictors, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c464b",
   "metadata": {},
   "source": [
    "Let's now visualize the workflow we've constructed so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba63303-b50d-4def-992e-aba7a6dea880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow parameters\n",
    "nsamples = 100\n",
    "size = 20\n",
    "dims = [2, 3]\n",
    "\n",
    "params = {\n",
    "    'nsamples': nsamples,\n",
    "    'num_elements': size,\n",
    "    'dimensions': dims\n",
    "}\n",
    "\n",
    "# Visualize workflow graph on the GUI\n",
    "dispatch_id = ct.dispatch(generate_training_data)(\n",
    "    nsamples=nsamples, \n",
    "    num_elements=size, \n",
    "    dimensions=dims\n",
    ")\n",
    "print(dispatch_id)\n",
    "\n",
    "# Navigate to localhost:48008 in your browser and examine the latest dispatch entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f167b",
   "metadata": {},
   "source": [
    "![Data generation workflow diagram](assets/qg_workflow_1.png)\n",
    "\n",
    "For any of these electrons, represented by nodes in the workflow graph, we can inspect status, query inputs and outputs, debug if there are error messages, and even re-run if a portion failed.  Features like these are useful if, for instance, we had a bug in the code which generated three-dimensional spacetimes.  In this case we could retain the correct two-dimensional data, make required changes to the code, and re-dispatch the lattice to complete the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2c6f6-716a-4544-9a96-ec49117ca176",
   "metadata": {},
   "source": [
    "## Constructing the Learning Model\n",
    "\n",
    "We continue now by constructing a learning model. We use a five-layer deep neural network trained with supervised learning. The network architecture is chosen such that the size of the layers uniformly decreases with depth, and the learning rate has been selected such that the training procedure converges in a reasonable time.  We also use batch training, i.e., the training operation occurs using randomized batches of 10% of the dataset each epoch, so that the model converges quickly and generalizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bfb45-6978-4650-9672-576ca16104cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the deep neural network\n",
    "\n",
    "# The number of input neurons is equal to the problem size\n",
    "num_inputs = size\n",
    "# The number of outputs is equal to the number of classes\n",
    "num_outputs = len(dims)\n",
    "# We use three hidden layers which decrease in size\n",
    "num_hidden = [ int(0.75*num_inputs), int(0.5*num_inputs), int(0.25*num_inputs) ]\n",
    "# Total number of layers in the neural network\n",
    "num_layers = 2 + len(num_hidden)\n",
    "# Layer sizes\n",
    "layer_sizes = [ num_inputs ] + num_hidden + [ num_outputs ]\n",
    "# Assert the sizes are decreasing\n",
    "assert(num_inputs > num_hidden[0])\n",
    "assert(num_hidden[-1] > num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0b889-7497-4abb-9088-f8ec831fd270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "@ct.electron\n",
    "def shuffle_training_data(\n",
    "    predictors: pd.DataFrame, \n",
    "    classes: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \n",
    "    num_inputs = len(predictors.columns)\n",
    "    num_outputs = len(classes.columns)\n",
    "    \n",
    "    shuffled_data = pd.concat([\n",
    "        predictors,\n",
    "        classes\n",
    "    ], axis=1).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    shuffled_predictors = shuffled_data[shuffled_data.columns[:num_inputs]]\n",
    "    shuffled_classes = shuffled_data[shuffled_data.columns[-num_outputs:]]\n",
    "    \n",
    "    return shuffled_predictors, shuffled_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed191dac-cc9a-44b6-a8dc-0a1554b27df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the learning model\n",
    "@ct.electron(executor=\"local\")\n",
    "def construct_model(\n",
    "    layers: List[int]\n",
    ") -> tf.keras.Model:\n",
    "    \n",
    "    # Model architecture described by layers\n",
    "    num_layers = len(layers)\n",
    "    num_inputs = layers[0]\n",
    "    num_outputs = layers[-1]\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(num_inputs,)))\n",
    "    for layer in range(num_layers - 1):\n",
    "        model.add(tf.keras.layers.Dense(layers[layer+1], activation=\"relu\"))\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2384b-0342-4078-9055-5dce9e760496",
   "metadata": {},
   "source": [
    "**Note**: We use the \"local\" executor for this electron because the default Dask-based executor sometimes fails to preserve object attributes when transferring data to and from worker processes. The underlying bug will be addressed in a future release. Should you encounter runtime issues, try setting `executor=\"local\"` for some electrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ad490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the model using synchronous dispatch\n",
    "model = ct.dispatch_sync(ct.lattice(construct_model))(layers=layer_sizes).result\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e2f0e1-20b2-45f5-a993-69693fd50b2e",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "In the next step, we train the model using the data we generated at the beginning.  We use a cross-entropy cost function and measure the accuracy every 100 epochs.  What we find is that learning occurs spontaneously, i.e., there is a jump in accuracy after some characteristic time which depends on the learning rate, system size, and number of training classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347595fa-a8f2-43a8-b80e-0052c5e64260",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def train_model(\n",
    "    model: tf.keras.Model, \n",
    "    predictors: pd.DataFrame, \n",
    "    classes: pd.DataFrame, \n",
    "    num_training_steps: int\n",
    ") -> tf.keras.callbacks.History:\n",
    "    \n",
    "    history = model.fit(\n",
    "        predictors, \n",
    "        classes, \n",
    "        epochs=num_training_steps\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6754321-7bbd-471c-bfb4-228f9efce0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning workflow\n",
    "@ct.lattice\n",
    "def spacetime_learning_workflow(\n",
    "    arch: List[int], \n",
    "    training_predictors: pd.DataFrame, \n",
    "    training_classes: pd.DataFrame, \n",
    "    num_training_steps: int\n",
    ") -> Tuple[tf.keras.Model, tf.keras.callbacks.History]:\n",
    "    \n",
    "    model = construct_model(layers=arch)\n",
    "    \n",
    "    history = train_model(\n",
    "        model=model, \n",
    "        predictors=training_predictors, \n",
    "        classes=training_classes, \n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1a97c",
   "metadata": {},
   "source": [
    "Before jumping to the final step, let's again examine the workflow in the user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictors, training_classes = generate_training_data(\n",
    "    nsamples=100, \n",
    "    num_elements=20, \n",
    "    dimensions=dims\n",
    ")\n",
    "\n",
    "ct.dispatch_sync(spacetime_learning_workflow)(\n",
    "    arch=layer_sizes, \n",
    "    training_predictors=training_predictors, \n",
    "    training_classes=training_classes, \n",
    "    num_training_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a36a5",
   "metadata": {},
   "source": [
    "![Spacetime learning workflow diagram](./assets/qg_workflow_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b420d-c009-4c92-9466-749e9cb63d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher-level workflow\n",
    "@ct.lattice\n",
    "def spacetime_classifier(\n",
    "    nsamples: int, \n",
    "    size: int, \n",
    "    dims: List[int], \n",
    "    num_training_steps: int\n",
    ") -> Tuple[tf.keras.Model, tf.keras.callbacks.History]:\n",
    "    \n",
    "    # Call the first sublattice, which generates training data\n",
    "    training_predictors, training_classes = ct.electron(generate_training_data)(\n",
    "        nsamples=nsamples, \n",
    "        num_elements=size, \n",
    "        dimensions=dims\n",
    "    )\n",
    "    \n",
    "    # Shuffle the training data\n",
    "    training_predictors, training_classes = shuffle_training_data(\n",
    "        predictors=training_predictors, \n",
    "        classes=training_classes\n",
    "    )\n",
    "    \n",
    "    # Call the next sublattice, which constructs and trains a learning model\n",
    "    model, history = ct.electron(spacetime_learning_workflow)(\n",
    "        arch=layer_sizes, \n",
    "        training_predictors=training_predictors, \n",
    "        training_classes=training_classes, \n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Dispatch the full workflow\n",
    "model, history = ct.dispatch_sync(spacetime_classifier)(\n",
    "    nsamples=nsamples, \n",
    "    size=size, \n",
    "    dims=dims, \n",
    "    num_training_steps=500\n",
    ").result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404040de",
   "metadata": {},
   "source": [
    "Let's inspect the final workflow.  Note that the nodes marked \"sublattice\" represent the two workflows shown in previous diagrams.\n",
    "\n",
    "![Spacetime classifier workflow](./assets/qg_workflow_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442c848",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Let's now look at the ability of the model to predict the outcome given new data.  We can first look at the training history to confirm the model has learned to categorize the training set properly.  We then generate a set of new samples and look at the accuracy of the classified results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1217d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.rcParams['figure.facecolor']='white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16778465",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def evaluate_model(\n",
    "    model: tf.keras.Model,\n",
    "    predictors: pd.DataFrame,\n",
    "    classes: pd.DataFrame\n",
    ") -> float:\n",
    "    \n",
    "    cost, accuracy = model.evaluate(predictors, classes)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Generate test data and test the model\n",
    "@ct.lattice\n",
    "def test_model(\n",
    "    model: tf.keras.Model, \n",
    "    nsamples: int, \n",
    "    num_elements: int, \n",
    "    dimensions: List[int]\n",
    ") -> float:\n",
    "    \n",
    "    testing_predictors, testing_classes = ct.electron(generate_training_data)(\n",
    "        nsamples=nsamples, num_elements=num_elements, dimensions=dimensions)\n",
    "    \n",
    "    testing_predictors, testing_classes = shuffle_training_data(\n",
    "        predictors=testing_predictors, \n",
    "        classes=testing_classes\n",
    "    )\n",
    "    \n",
    "    test_accuracy = evaluate_model(\n",
    "        model=model, \n",
    "        predictors=testing_predictors, \n",
    "        classes=testing_classes\n",
    "    )\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "accuracy = ct.dispatch_sync(test_model)(\n",
    "    model=model, \n",
    "    nsamples=nsamples, \n",
    "    num_elements=size, \n",
    "    dimensions=dims\n",
    ").result\n",
    "accuracy = test_model(model=model, nsamples=nsamples, num_elements=size, dimensions=dims)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76829e96",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial we used Covalent in a variety of ways. In several sections we constructed workflows which we later combined into a larger workflow by making them sublattices.  Using the Covalent UI, we were able to visualize and track the progress of the experiments. An advanced user could even continue along the same path to examine how the number of epochs needed to learn to classify can change with the problem size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
