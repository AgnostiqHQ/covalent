{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classifier tutorial\n",
    "\n",
    "* MNIST database of handwritten digits is a popular dataset to demonstrate machine learning classifier training.\n",
    "* In this tutorial, we train a basic Neural Network (NN) classifier using PyTorch.\n",
    "* Once the basic NN classifier training workflow has been defined, we parallelize the mutually independent tasks using Covalent.\n",
    "* This tutorial has two primary objectives.\n",
    "* One goal is to show the ease with which a \"normal\" workflow can be adapted to a Covalent workflow.\n",
    "* The second objective is to display the browser-based Covalent workflow tracking UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cova\n",
    "# !conda install pytorch torchvision -c pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covalent as ct\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fe3c046c6d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Add global variables inside the workflow \n",
    "# n_epochs = 2\n",
    "# batch_size_train = 64\n",
    "# batch_size_test = 1000\n",
    "# learning_rate = 0.01\n",
    "# momentum = 0.5\n",
    "# log_interval = 10\n",
    "\n",
    "# random_seed = 1\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.manual_seed(random_seed)\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# network = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def data_loader(batch_size: int,\n",
    "                train: bool,\n",
    "                download: bool = True,\n",
    "                shuffle: bool = True,\n",
    "                data_dir: str = '~/data/mnist/',\n",
    "                ) -> torch.utils.data.dataloader.DataLoader:\n",
    "    \"\"\"MNIST data loader.\"\"\"\n",
    "\n",
    "    data_dir = Path(data_dir).expanduser()\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data = datasets.MNIST(\n",
    "        data_dir,\n",
    "        train=train,\n",
    "        download=download,\n",
    "        transform=ToTensor()\n",
    "        )\n",
    "\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_optimizer(model: NeuralNetwork, learning_rate: float, momentum: float) -> \\\n",
    "        torch.optim.Optimizer:\n",
    "    \"\"\"Get Stochastic Gradient Descent optimizer.\"\"\"\n",
    "\n",
    "    return torch.optim.SGD(model.parameters(), learning_rate, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def train_over_one_epoch(dataloader: torch.utils.data.dataloader.DataLoader,\n",
    "                         model: NeuralNetwork,\n",
    "                         optimizer: torch.optim.Optimizer,\n",
    "                         log_interval: int,\n",
    "                         epoch: int,\n",
    "                         loss_fn,\n",
    "                         train_losses: list,\n",
    "                         train_counter: int,\n",
    "                         device: str = \"cpu\"):\n",
    "    \"\"\"Train neural network model over one epoch.\"\"\"\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % log_interval == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "            train_losses.append(loss)\n",
    "            train_counter.append(\n",
    "                (batch * 64) + ((epoch - 1) * len(dataloader.dataset)))\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def test(dataloader: torch.utils.data.dataloader.DataLoader,\n",
    "         model: NeuralNetwork,\n",
    "         loss_fn: callable,\n",
    "         device: str = \"cpu\") -> None:\n",
    "    \"\"\"Test the model performance.\"\"\"\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def train_model(train_dataloader: torch.utils.data.dataloader.DataLoader,\n",
    "                train_losses: list,\n",
    "                train_counter: int,\n",
    "                log_interval: int,\n",
    "                model: NeuralNetwork,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                loss_fn: callable,\n",
    "                epochs: int,\n",
    "                results_dir: str = '~/data/mnist/results/') -> Tuple[NeuralNetwork, ]:\n",
    "    \"\"\"Train neural network model.\"\"\"\n",
    "\n",
    "    results_dir = Path(results_dir).expanduser()\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "        model, optimizer = train_over_one_epoch(dataloader=train_dataloader,\n",
    "                                                model=model,\n",
    "                                                optimizer=optimizer,\n",
    "                                                train_losses=train_losses,\n",
    "                                                train_counter=train_counter,\n",
    "                                                log_interval=log_interval,\n",
    "                                                epoch=epoch,\n",
    "                                                loss_fn=loss_fn)\n",
    "\n",
    "    # Save model and optimizer\n",
    "    torch.save(model.state_dict(), f'{results_dir}model.pth')\n",
    "    torch.save(optimizer.state_dict(), f'{results_dir}optimizer.pth')\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def workflow(model: NeuralNetwork,\n",
    "             epochs: int = 2,\n",
    "             batch_size_train: int = 64,\n",
    "             batch_size_test: int = 1000,\n",
    "             learning_rate: float = 0.01,\n",
    "             momentum: float = 0.5,\n",
    "             log_interval: int = 10,\n",
    "             loss_fn: callable = F.nll_loss):\n",
    "    \"\"\"MNIST classifier training workflow\"\"\"\n",
    "\n",
    "    train_dataloader = data_loader(batch_size=batch_size_train, train=True)\n",
    "    test_dataloader = data_loader(batch_size=batch_size_test, train=False)\n",
    "\n",
    "    train_losses, train_counter, test_losses = [], [], []\n",
    "    optimizer = get_optimizer(model=model, learning_rate=learning_rate, momentum=momentum)\n",
    "    model, optimizer = train_model(\n",
    "        train_dataloader=train_dataloader,\n",
    "        train_losses=train_losses,\n",
    "        train_counter=train_counter,\n",
    "        log_interval=log_interval,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        epochs=epochs)\n",
    "    test(dataloader=test_dataloader, model=model, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.320409  [    0/60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hn/dnjxkq7946j9wpxv1cy9pm2m0000gn/T/ipykernel_70432/4014429098.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.307707  [  640/60000]\n",
      "loss: 2.311337  [ 1280/60000]\n",
      "loss: 2.311996  [ 1920/60000]\n",
      "loss: 2.303629  [ 2560/60000]\n",
      "loss: 2.306665  [ 3200/60000]\n",
      "loss: 2.290847  [ 3840/60000]\n",
      "loss: 2.314099  [ 4480/60000]\n",
      "loss: 2.291214  [ 5120/60000]\n",
      "loss: 2.301382  [ 5760/60000]\n",
      "loss: 2.289404  [ 6400/60000]\n",
      "loss: 2.291337  [ 7040/60000]\n",
      "loss: 2.302155  [ 7680/60000]\n",
      "loss: 2.301734  [ 8320/60000]\n",
      "loss: 2.291988  [ 8960/60000]\n",
      "loss: 2.279692  [ 9600/60000]\n",
      "loss: 2.286411  [10240/60000]\n",
      "loss: 2.284787  [10880/60000]\n",
      "loss: 2.287410  [11520/60000]\n",
      "loss: 2.276541  [12160/60000]\n",
      "loss: 2.269667  [12800/60000]\n",
      "loss: 2.275173  [13440/60000]\n",
      "loss: 2.300417  [14080/60000]\n",
      "loss: 2.263923  [14720/60000]\n",
      "loss: 2.266652  [15360/60000]\n",
      "loss: 2.251887  [16000/60000]\n",
      "loss: 2.253369  [16640/60000]\n",
      "loss: 2.259032  [17280/60000]\n",
      "loss: 2.251317  [17920/60000]\n",
      "loss: 2.242465  [18560/60000]\n",
      "loss: 2.242042  [19200/60000]\n",
      "loss: 2.176988  [19840/60000]\n",
      "loss: 2.204401  [20480/60000]\n",
      "loss: 2.224599  [21120/60000]\n",
      "loss: 2.214455  [21760/60000]\n",
      "loss: 2.159513  [22400/60000]\n",
      "loss: 2.137746  [23040/60000]\n",
      "loss: 2.070199  [23680/60000]\n",
      "loss: 2.041916  [24320/60000]\n",
      "loss: 2.056910  [24960/60000]\n",
      "loss: 1.959934  [25600/60000]\n",
      "loss: 1.927880  [26240/60000]\n",
      "loss: 1.960632  [26880/60000]\n",
      "loss: 1.743099  [27520/60000]\n",
      "loss: 1.883952  [28160/60000]\n",
      "loss: 1.709106  [28800/60000]\n",
      "loss: 1.750584  [29440/60000]\n",
      "loss: 1.521725  [30080/60000]\n",
      "loss: 1.722037  [30720/60000]\n",
      "loss: 1.656173  [31360/60000]\n",
      "loss: 1.669859  [32000/60000]\n",
      "loss: 1.451845  [32640/60000]\n",
      "loss: 1.402583  [33280/60000]\n",
      "loss: 1.249204  [33920/60000]\n",
      "loss: 1.188051  [34560/60000]\n",
      "loss: 1.342556  [35200/60000]\n",
      "loss: 1.305990  [35840/60000]\n",
      "loss: 1.318674  [36480/60000]\n",
      "loss: 1.167150  [37120/60000]\n",
      "loss: 1.151486  [37760/60000]\n",
      "loss: 0.989652  [38400/60000]\n",
      "loss: 1.054066  [39040/60000]\n",
      "loss: 1.282395  [39680/60000]\n",
      "loss: 1.124953  [40320/60000]\n",
      "loss: 1.090747  [40960/60000]\n",
      "loss: 0.958166  [41600/60000]\n",
      "loss: 0.944912  [42240/60000]\n",
      "loss: 0.873774  [42880/60000]\n",
      "loss: 0.964176  [43520/60000]\n",
      "loss: 0.847067  [44160/60000]\n",
      "loss: 0.716929  [44800/60000]\n",
      "loss: 0.995187  [45440/60000]\n",
      "loss: 0.887824  [46080/60000]\n",
      "loss: 0.650816  [46720/60000]\n",
      "loss: 0.770471  [47360/60000]\n",
      "loss: 0.903852  [48000/60000]\n",
      "loss: 0.826277  [48640/60000]\n",
      "loss: 1.438468  [49280/60000]\n",
      "loss: 0.902839  [49920/60000]\n",
      "loss: 0.930190  [50560/60000]\n",
      "loss: 0.969953  [51200/60000]\n",
      "loss: 0.955306  [51840/60000]\n",
      "loss: 1.008514  [52480/60000]\n",
      "loss: 0.852795  [53120/60000]\n",
      "loss: 0.596097  [53760/60000]\n",
      "loss: 0.793817  [54400/60000]\n",
      "loss: 0.836756  [55040/60000]\n",
      "loss: 0.762055  [55680/60000]\n",
      "loss: 0.645518  [56320/60000]\n",
      "loss: 0.657663  [56960/60000]\n",
      "loss: 0.794816  [57600/60000]\n",
      "loss: 0.645152  [58240/60000]\n",
      "loss: 0.879877  [58880/60000]\n",
      "loss: 0.661890  [59520/60000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.600443  [    0/60000]\n",
      "loss: 1.063987  [  640/60000]\n",
      "loss: 0.732200  [ 1280/60000]\n",
      "loss: 0.816915  [ 1920/60000]\n",
      "loss: 0.507861  [ 2560/60000]\n",
      "loss: 0.484259  [ 3200/60000]\n",
      "loss: 0.753194  [ 3840/60000]\n",
      "loss: 0.695168  [ 4480/60000]\n",
      "loss: 0.764311  [ 5120/60000]\n",
      "loss: 0.779839  [ 5760/60000]\n",
      "loss: 0.553753  [ 6400/60000]\n",
      "loss: 0.714791  [ 7040/60000]\n",
      "loss: 0.533148  [ 7680/60000]\n",
      "loss: 0.774047  [ 8320/60000]\n",
      "loss: 0.704335  [ 8960/60000]\n",
      "loss: 0.649910  [ 9600/60000]\n",
      "loss: 0.658484  [10240/60000]\n",
      "loss: 0.679896  [10880/60000]\n",
      "loss: 0.688639  [11520/60000]\n",
      "loss: 0.546472  [12160/60000]\n",
      "loss: 0.686799  [12800/60000]\n",
      "loss: 0.564192  [13440/60000]\n",
      "loss: 0.629861  [14080/60000]\n",
      "loss: 0.913170  [14720/60000]\n",
      "loss: 0.943313  [15360/60000]\n",
      "loss: 0.566532  [16000/60000]\n",
      "loss: 0.581406  [16640/60000]\n",
      "loss: 0.610642  [17280/60000]\n",
      "loss: 0.456867  [17920/60000]\n",
      "loss: 0.570269  [18560/60000]\n",
      "loss: 0.903296  [19200/60000]\n",
      "loss: 0.523997  [19840/60000]\n",
      "loss: 0.547268  [20480/60000]\n",
      "loss: 0.460541  [21120/60000]\n",
      "loss: 0.673987  [21760/60000]\n",
      "loss: 0.650082  [22400/60000]\n",
      "loss: 0.622396  [23040/60000]\n",
      "loss: 0.647220  [23680/60000]\n",
      "loss: 0.481297  [24320/60000]\n",
      "loss: 0.803640  [24960/60000]\n",
      "loss: 0.598102  [25600/60000]\n",
      "loss: 0.594031  [26240/60000]\n",
      "loss: 0.527483  [26880/60000]\n",
      "loss: 0.481619  [27520/60000]\n",
      "loss: 0.493521  [28160/60000]\n",
      "loss: 0.772172  [28800/60000]\n",
      "loss: 0.441312  [29440/60000]\n",
      "loss: 0.401709  [30080/60000]\n",
      "loss: 0.400059  [30720/60000]\n",
      "loss: 0.893787  [31360/60000]\n",
      "loss: 0.609976  [32000/60000]\n",
      "loss: 0.544402  [32640/60000]\n",
      "loss: 0.343184  [33280/60000]\n",
      "loss: 0.480678  [33920/60000]\n",
      "loss: 0.671037  [34560/60000]\n",
      "loss: 0.372365  [35200/60000]\n",
      "loss: 0.484423  [35840/60000]\n",
      "loss: 0.719567  [36480/60000]\n",
      "loss: 0.620435  [37120/60000]\n",
      "loss: 0.581285  [37760/60000]\n",
      "loss: 0.573992  [38400/60000]\n",
      "loss: 0.465196  [39040/60000]\n",
      "loss: 0.370556  [39680/60000]\n",
      "loss: 0.413437  [40320/60000]\n",
      "loss: 0.517824  [40960/60000]\n",
      "loss: 0.511812  [41600/60000]\n",
      "loss: 0.265565  [42240/60000]\n",
      "loss: 0.402354  [42880/60000]\n",
      "loss: 0.610592  [43520/60000]\n",
      "loss: 0.450277  [44160/60000]\n",
      "loss: 0.418185  [44800/60000]\n",
      "loss: 0.722991  [45440/60000]\n",
      "loss: 0.489526  [46080/60000]\n",
      "loss: 0.513264  [46720/60000]\n",
      "loss: 0.480109  [47360/60000]\n",
      "loss: 0.394789  [48000/60000]\n",
      "loss: 0.542933  [48640/60000]\n",
      "loss: 0.631011  [49280/60000]\n",
      "loss: 0.502751  [49920/60000]\n",
      "loss: 0.546607  [50560/60000]\n",
      "loss: 0.407683  [51200/60000]\n",
      "loss: 0.411038  [51840/60000]\n",
      "loss: 0.457658  [52480/60000]\n",
      "loss: 0.508973  [53120/60000]\n",
      "loss: 0.667909  [53760/60000]\n",
      "loss: 0.356546  [54400/60000]\n",
      "loss: 0.509483  [55040/60000]\n",
      "loss: 0.407351  [55680/60000]\n",
      "loss: 0.422807  [56320/60000]\n",
      "loss: 0.514971  [56960/60000]\n",
      "loss: 0.439938  [57600/60000]\n",
      "loss: 0.687930  [58240/60000]\n",
      "loss: 0.300058  [58880/60000]\n",
      "loss: 0.335240  [59520/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.218073 \n",
      "\n",
      "Regular workflow takes 40.410712003707886 seconds.\n"
     ]
    }
   ],
   "source": [
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    workflow(model=NeuralNetwork().to(\"cpu\"),)\n",
    "    end = time.time()\n",
    "    print(f\"Regular workflow takes {end - start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3fb152ec-da1c-4149-a3c0-4bab582b44ae\n",
      "None\n",
      "Covalent workflow execution time: 37.76052188873291\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dispatch_id = ct.dispatch(workflow)()\n",
    "print(dispatch_id)\n",
    "result = ct.get_result(dispatch_id=dispatch_id, wait=True)\n",
    "print(result.result)\n",
    "end = time.time()\n",
    "print(f\"Covalent workflow execution time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bae16d96-3deb-4894-b016-b1a2500d4a29\n",
      "None\n",
      "Covalent workflow takes 45.22564506530762 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dispatch_id = ct.dispatch(workflow)(model=NeuralNetwork().to(\"cpu\"),)\n",
    "print(dispatch_id)\n",
    "result = ct.get_result(dispatch_id=dispatch_id, wait=True)\n",
    "print(result.result)\n",
    "end = time.time()\n",
    "print(f\"Covalent workflow takes {end - start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triple check to ensure there is no plagiarism issues\n",
    "References:\n",
    "- https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "497139f919dadc5e613ed634ff9107622fdaea40400342bdf68cd50a71d28343"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('covalent-pytorch-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
