{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classifier tutorial\n",
    "\n",
    "* MNIST database of handwritten digits is a popular dataset to demonstrate machine learning classifier training.\n",
    "* In this tutorial, we train a basic Neural Network (NN) classifier using PyTorch.\n",
    "* Once the basic NN classifier training workflow has been defined, it is easily converted to a Covalent workflow.\n",
    "* This tutorial has two primary objectives.\n",
    "* One goal is to show the ease with which a \"normal\" workflow can be adapted to a Covalent workflow.\n",
    "* The second objective is to display the browser-based Covalent workflow tracking UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "This tutorial requires installing Covalent, PyTorch and Torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cova\n",
    "# !conda install pytorch torchvision -c pytorch -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once Covalent has been installed, run `covalent start` to start the dispatcher and browser-based UI servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !covalent start   # Start the server\n",
    "# !covalent status  # Check server status\n",
    "# !covalent stop    # Stop the server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Covalent, PyTorch and other relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covalent as ct\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct MNIST classifier training workflow\n",
    "\n",
    "Construct a convolutional neural network model by inheriting from `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a data loader to retrieve the classifier training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def data_loader(\n",
    "    batch_size: int,\n",
    "    train: bool,\n",
    "    download: bool = True,\n",
    "    shuffle: bool = True,\n",
    "    data_dir: str = \"~/data/mnist/\",\n",
    ") -> torch.utils.data.dataloader.DataLoader:\n",
    "    \"\"\"MNIST data loader.\"\"\"\n",
    "\n",
    "    data_dir = Path(data_dir).expanduser()\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data = datasets.MNIST(data_dir, train=train, download=download, transform=ToTensor())\n",
    "\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a function to retrieve a Stochastic Gradient Descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_optimizer(\n",
    "    model: NeuralNetwork, learning_rate: float, momentum: float\n",
    ") -> torch.optim.Optimizer:\n",
    "    \"\"\"Get Stochastic Gradient Descent optimizer.\"\"\"\n",
    "\n",
    "    return torch.optim.SGD(model.parameters(), learning_rate, momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to train the model over one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def train_over_one_epoch(\n",
    "    dataloader: torch.utils.data.dataloader.DataLoader,\n",
    "    model: NeuralNetwork,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    log_interval: int,\n",
    "    epoch: int,\n",
    "    loss_fn,\n",
    "    train_losses: list,\n",
    "    train_counter: int,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"Train neural network model over one epoch.\"\"\"\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % log_interval == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "            train_losses.append(loss)\n",
    "            train_counter.append((batch * 64) + ((epoch - 1) * len(dataloader.dataset)))\n",
    "\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to test the performance of the classifier for a given loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def test(\n",
    "    dataloader: torch.utils.data.dataloader.DataLoader,\n",
    "    model: NeuralNetwork,\n",
    "    loss_fn: callable,\n",
    "    device: str = \"cpu\",\n",
    ") -> None:\n",
    "    \"\"\"Test the model performance.\"\"\"\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to train the model over several epochs and save the final state of the _optimizer_ and the _neural network_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def train_model(\n",
    "    train_dataloader: torch.utils.data.dataloader.DataLoader,\n",
    "    train_losses: list,\n",
    "    train_counter: int,\n",
    "    log_interval: int,\n",
    "    model: NeuralNetwork,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: callable,\n",
    "    epochs: int,\n",
    "    results_dir: str = \"~/data/mnist/results/\",\n",
    ") -> Tuple[NeuralNetwork,]:\n",
    "    \"\"\"Train neural network model.\"\"\"\n",
    "\n",
    "    results_dir = Path(results_dir).expanduser()\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "        model, optimizer = train_over_one_epoch(\n",
    "            dataloader=train_dataloader,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_losses=train_losses,\n",
    "            train_counter=train_counter,\n",
    "            log_interval=log_interval,\n",
    "            epoch=epoch,\n",
    "            loss_fn=loss_fn,\n",
    "        )\n",
    "\n",
    "    # Save model and optimizer\n",
    "    torch.save(model.state_dict(), f\"{results_dir}model.pth\")\n",
    "    torch.save(optimizer.state_dict(), f\"{results_dir}optimizer.pth\")\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we put all these tasks together to construct the MNIST classifier training and test workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def workflow(\n",
    "    model: NeuralNetwork,\n",
    "    epochs: int = 2,\n",
    "    batch_size_train: int = 64,\n",
    "    batch_size_test: int = 1000,\n",
    "    learning_rate: float = 0.01,\n",
    "    momentum: float = 0.5,\n",
    "    log_interval: int = 10,\n",
    "    loss_fn: callable = F.nll_loss,\n",
    "):\n",
    "    \"\"\"MNIST classifier training workflow\"\"\"\n",
    "\n",
    "    train_dataloader = data_loader(batch_size=batch_size_train, train=True)\n",
    "    test_dataloader = data_loader(batch_size=batch_size_test, train=False)\n",
    "\n",
    "    train_losses, train_counter, test_losses = [], [], []\n",
    "    optimizer = get_optimizer(model=model, learning_rate=learning_rate, momentum=momentum)\n",
    "    model, optimizer = train_model(\n",
    "        train_dataloader=train_dataloader,\n",
    "        train_losses=train_losses,\n",
    "        train_counter=train_counter,\n",
    "        log_interval=log_interval,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    test(dataloader=test_dataloader, model=model, loss_fn=loss_fn)\n",
    "\n",
    "    return train_losses, train_counter, test_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MNIST classifier workflow as a normal function\n",
    "\n",
    "Run the MNIST classifier workflow to benchmark the performance and the time taken to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.315409  [    0/60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hn/dnjxkq7946j9wpxv1cy9pm2m0000gn/T/ipykernel_34544/3977843674.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.307642  [  640/60000]\n",
      "loss: 2.296734  [ 1280/60000]\n",
      "loss: 2.307033  [ 1920/60000]\n",
      "loss: 2.296630  [ 2560/60000]\n",
      "loss: 2.302155  [ 3200/60000]\n",
      "loss: 2.301476  [ 3840/60000]\n",
      "loss: 2.288598  [ 4480/60000]\n",
      "loss: 2.287490  [ 5120/60000]\n",
      "loss: 2.278107  [ 5760/60000]\n",
      "loss: 2.276480  [ 6400/60000]\n",
      "loss: 2.271002  [ 7040/60000]\n",
      "loss: 2.287921  [ 7680/60000]\n",
      "loss: 2.281850  [ 8320/60000]\n",
      "loss: 2.280744  [ 8960/60000]\n",
      "loss: 2.256821  [ 9600/60000]\n",
      "loss: 2.245662  [10240/60000]\n",
      "loss: 2.231503  [10880/60000]\n",
      "loss: 2.219224  [11520/60000]\n",
      "loss: 2.215914  [12160/60000]\n",
      "loss: 2.193003  [12800/60000]\n",
      "loss: 2.168643  [13440/60000]\n",
      "loss: 2.137305  [14080/60000]\n",
      "loss: 2.178993  [14720/60000]\n",
      "loss: 2.079994  [15360/60000]\n",
      "loss: 2.111959  [16000/60000]\n",
      "loss: 1.995600  [16640/60000]\n",
      "loss: 2.014947  [17280/60000]\n",
      "loss: 1.868428  [17920/60000]\n",
      "loss: 1.680940  [18560/60000]\n",
      "loss: 1.876748  [19200/60000]\n",
      "loss: 1.665180  [19840/60000]\n",
      "loss: 1.683950  [20480/60000]\n",
      "loss: 1.444181  [21120/60000]\n",
      "loss: 1.597151  [21760/60000]\n",
      "loss: 1.355121  [22400/60000]\n",
      "loss: 1.539470  [23040/60000]\n",
      "loss: 1.321967  [23680/60000]\n",
      "loss: 1.331592  [24320/60000]\n",
      "loss: 1.329327  [24960/60000]\n",
      "loss: 1.297047  [25600/60000]\n",
      "loss: 1.028821  [26240/60000]\n",
      "loss: 0.998983  [26880/60000]\n",
      "loss: 1.386499  [27520/60000]\n",
      "loss: 1.105137  [28160/60000]\n",
      "loss: 1.108799  [28800/60000]\n",
      "loss: 0.861200  [29440/60000]\n",
      "loss: 1.036322  [30080/60000]\n",
      "loss: 0.916468  [30720/60000]\n",
      "loss: 0.837160  [31360/60000]\n",
      "loss: 0.955477  [32000/60000]\n",
      "loss: 0.955188  [32640/60000]\n",
      "loss: 1.116368  [33280/60000]\n",
      "loss: 0.924046  [33920/60000]\n",
      "loss: 0.837326  [34560/60000]\n",
      "loss: 0.701483  [35200/60000]\n",
      "loss: 0.907916  [35840/60000]\n",
      "loss: 0.939573  [36480/60000]\n",
      "loss: 1.023081  [37120/60000]\n",
      "loss: 0.743001  [37760/60000]\n",
      "loss: 0.785731  [38400/60000]\n",
      "loss: 0.925454  [39040/60000]\n",
      "loss: 1.045340  [39680/60000]\n",
      "loss: 0.866044  [40320/60000]\n",
      "loss: 0.741980  [40960/60000]\n",
      "loss: 0.745667  [41600/60000]\n",
      "loss: 0.990555  [42240/60000]\n",
      "loss: 0.885761  [42880/60000]\n",
      "loss: 0.756008  [43520/60000]\n",
      "loss: 0.912831  [44160/60000]\n",
      "loss: 0.987226  [44800/60000]\n",
      "loss: 0.843590  [45440/60000]\n",
      "loss: 0.577257  [46080/60000]\n",
      "loss: 0.587669  [46720/60000]\n",
      "loss: 0.837506  [47360/60000]\n",
      "loss: 0.763376  [48000/60000]\n",
      "loss: 0.639798  [48640/60000]\n",
      "loss: 0.822364  [49280/60000]\n",
      "loss: 0.610434  [49920/60000]\n",
      "loss: 0.731896  [50560/60000]\n",
      "loss: 0.671959  [51200/60000]\n",
      "loss: 0.601348  [51840/60000]\n",
      "loss: 0.700470  [52480/60000]\n",
      "loss: 0.673797  [53120/60000]\n",
      "loss: 0.598445  [53760/60000]\n",
      "loss: 0.562891  [54400/60000]\n",
      "loss: 0.608531  [55040/60000]\n",
      "loss: 0.532839  [55680/60000]\n",
      "loss: 0.712112  [56320/60000]\n",
      "loss: 0.652364  [56960/60000]\n",
      "loss: 0.679695  [57600/60000]\n",
      "loss: 0.776890  [58240/60000]\n",
      "loss: 0.557132  [58880/60000]\n",
      "loss: 0.686152  [59520/60000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.640795  [    0/60000]\n",
      "loss: 0.554220  [  640/60000]\n",
      "loss: 0.496221  [ 1280/60000]\n",
      "loss: 0.816696  [ 1920/60000]\n",
      "loss: 0.507635  [ 2560/60000]\n",
      "loss: 0.540216  [ 3200/60000]\n",
      "loss: 0.447628  [ 3840/60000]\n",
      "loss: 0.617459  [ 4480/60000]\n",
      "loss: 0.594526  [ 5120/60000]\n",
      "loss: 0.470528  [ 5760/60000]\n",
      "loss: 0.791787  [ 6400/60000]\n",
      "loss: 0.717417  [ 7040/60000]\n",
      "loss: 0.430002  [ 7680/60000]\n",
      "loss: 0.634548  [ 8320/60000]\n",
      "loss: 0.521685  [ 8960/60000]\n",
      "loss: 0.569753  [ 9600/60000]\n",
      "loss: 0.479641  [10240/60000]\n",
      "loss: 0.577858  [10880/60000]\n",
      "loss: 0.731189  [11520/60000]\n",
      "loss: 0.884006  [12160/60000]\n",
      "loss: 0.775966  [12800/60000]\n",
      "loss: 0.408254  [13440/60000]\n",
      "loss: 0.714375  [14080/60000]\n",
      "loss: 0.552584  [14720/60000]\n",
      "loss: 0.757670  [15360/60000]\n",
      "loss: 0.497016  [16000/60000]\n",
      "loss: 0.533082  [16640/60000]\n",
      "loss: 0.443098  [17280/60000]\n",
      "loss: 0.404980  [17920/60000]\n",
      "loss: 0.566815  [18560/60000]\n",
      "loss: 0.805047  [19200/60000]\n",
      "loss: 0.895860  [19840/60000]\n",
      "loss: 0.325949  [20480/60000]\n",
      "loss: 0.373040  [21120/60000]\n",
      "loss: 0.332009  [21760/60000]\n",
      "loss: 0.545972  [22400/60000]\n",
      "loss: 0.392816  [23040/60000]\n",
      "loss: 0.556813  [23680/60000]\n",
      "loss: 0.405701  [24320/60000]\n",
      "loss: 0.658474  [24960/60000]\n",
      "loss: 0.439874  [25600/60000]\n",
      "loss: 0.506537  [26240/60000]\n",
      "loss: 0.305105  [26880/60000]\n",
      "loss: 0.523341  [27520/60000]\n",
      "loss: 0.457845  [28160/60000]\n",
      "loss: 0.440756  [28800/60000]\n",
      "loss: 0.590181  [29440/60000]\n",
      "loss: 0.531017  [30080/60000]\n",
      "loss: 0.686168  [30720/60000]\n",
      "loss: 0.409196  [31360/60000]\n",
      "loss: 0.291774  [32000/60000]\n",
      "loss: 0.584441  [32640/60000]\n",
      "loss: 0.365399  [33280/60000]\n",
      "loss: 0.329182  [33920/60000]\n",
      "loss: 0.425235  [34560/60000]\n",
      "loss: 0.600850  [35200/60000]\n",
      "loss: 0.480150  [35840/60000]\n",
      "loss: 0.618086  [36480/60000]\n",
      "loss: 0.434424  [37120/60000]\n",
      "loss: 0.354623  [37760/60000]\n",
      "loss: 0.695338  [38400/60000]\n",
      "loss: 0.496834  [39040/60000]\n",
      "loss: 0.352832  [39680/60000]\n",
      "loss: 0.589210  [40320/60000]\n",
      "loss: 0.315697  [40960/60000]\n",
      "loss: 0.519151  [41600/60000]\n",
      "loss: 0.468274  [42240/60000]\n",
      "loss: 0.606243  [42880/60000]\n",
      "loss: 0.501571  [43520/60000]\n",
      "loss: 0.294345  [44160/60000]\n",
      "loss: 0.513462  [44800/60000]\n",
      "loss: 0.217562  [45440/60000]\n",
      "loss: 0.579335  [46080/60000]\n",
      "loss: 0.573546  [46720/60000]\n",
      "loss: 0.293409  [47360/60000]\n",
      "loss: 0.559874  [48000/60000]\n",
      "loss: 0.416953  [48640/60000]\n",
      "loss: 0.530578  [49280/60000]\n",
      "loss: 0.478569  [49920/60000]\n",
      "loss: 0.893238  [50560/60000]\n",
      "loss: 0.408205  [51200/60000]\n",
      "loss: 0.477486  [51840/60000]\n",
      "loss: 0.513188  [52480/60000]\n",
      "loss: 0.573624  [53120/60000]\n",
      "loss: 0.780420  [53760/60000]\n",
      "loss: 0.366428  [54400/60000]\n",
      "loss: 0.477301  [55040/60000]\n",
      "loss: 0.428528  [55680/60000]\n",
      "loss: 0.638998  [56320/60000]\n",
      "loss: 0.261118  [56960/60000]\n",
      "loss: 0.554702  [57600/60000]\n",
      "loss: 0.418360  [58240/60000]\n",
      "loss: 0.482195  [58880/60000]\n",
      "loss: 0.402714  [59520/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.194898 \n",
      "\n",
      "Regular workflow takes 31.27734613418579 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "workflow(\n",
    "    model=NeuralNetwork().to(\"cpu\"),\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Regular workflow takes {end - start} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"././mnist_images/regular_workflow_loss.png\"  width=\"95%\" height=\"95%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run workflow with Covalent\n",
    "\n",
    "The Covalent dispatcher (`ct.dispatch`) can be used to dispatch the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispatch id: 04698a23-6854-4f6b-8094-0dd49014aa71\n",
      "Covalent workflow takes 0:00:40.276719 seconds.\n"
     ]
    }
   ],
   "source": [
    "dispatch_id = ct.dispatch(workflow)(\n",
    "    model=NeuralNetwork().to(\"cpu\"),\n",
    ")\n",
    "print(f\"Dispatch id: {dispatch_id}\")\n",
    "result = ct.get_result(dispatch_id=dispatch_id, wait=True)\n",
    "print(f\"Covalent workflow takes {result.end_time - result.start_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lattice Result\n",
      "==============\n",
      "status: COMPLETED\n",
      "result: ([], [], [])\n",
      "inputs: {'args': [], 'kwargs': {'model': NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")}}\n",
      "error: None\n",
      "\n",
      "start_time: 2022-02-27 21:30:58.610040+00:00\n",
      "end_time: 2022-02-27 21:31:38.886759+00:00\n",
      "\n",
      "results_dir: /Users/faiyaz/Code/covalent/doc/source/tutorials/machine_learning/results\n",
      "dispatch_id: 04698a23-6854-4f6b-8094-0dd49014aa71\n",
      "\n",
      "Node Outputs\n",
      "------------\n",
      "data_loader(0): <torch.utils.data.dataloader.DataLoader object at 0x7fec204edeb0>\n",
      ":parameter:64(1): 64\n",
      ":parameter:True(2): True\n",
      "data_loader(3): <torch.utils.data.dataloader.DataLoader object at 0x7fec204cf400>\n",
      ":parameter:1000(4): 1000\n",
      ":parameter:False(5): False\n",
      "get_optimizer(6): SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.5\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      ":parameter:NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")(7): NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      ":parameter:0.01(8): 0.01\n",
      ":parameter:0.5(9): 0.5\n",
      "train_model(10): (NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.5\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      "))\n",
      ":electron_list:(11): []\n",
      ":electron_list:(12): []\n",
      ":parameter:10(13): 10\n",
      ":parameter:NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")(14): NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      ":parameter:<function nll_loss at 0x7fec4329aaf0>(15): <function nll_loss at 0x7fec4329aaf0>\n",
      ":parameter:2(16): 2\n",
      ":generated:train_model()[0](17): NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      ":generated:train_model()[1](18): SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.5\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "test(19): None\n",
      ":parameter:<function nll_loss at 0x7fec4329aaf0>(20): <function nll_loss at 0x7fec4329aaf0>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the workflow has been dispatched, the results can be tracked on the covalent UI browser. Use `covalent status` (shown below) to find the UI browser address. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covalent server is running at http://0.0.0.0:48008.\n"
     ]
    }
   ],
   "source": [
    "!covalent status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking on `http://0.0.0.0:48008` we can see the UI browser which lists the various dispatch ids.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"././mnist_images/ui_dispatch_ids.png\"  width=\"95%\" height=\"95%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking on the dispatch id, we can see the details of the workflow execution. Note the task execution dependency graph. \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"././mnist_images/ui_workflow.png\"  width=\"95%\" height=\"95%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covalent concepts\n",
    "\n",
    "* Covalent allows the user to deploy multiple workflows without having to wait for them to finish running. [Supports asynchronous workflow deployment without any additional code???]\n",
    "\n",
    "* A Covalent workflow can be _dispatched_ to take advantage of automatic parallelization, user friendly interface etc. but it can also just as easily be run as a normal Python function. Hence, adding the electron / lattice decorators only enhances what we can be done with the workflow with a minimum overhead.\n",
    "\n",
    "* Execution time for Covalent workflows and subtasks are readily available without needing any additional code.\n",
    "\n",
    "References:\n",
    "- https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "497139f919dadc5e613ed634ff9107622fdaea40400342bdf68cd50a71d28343"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('covalent-pytorch-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
