{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import covalent as ct\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ParameterGrid\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = ct.executor.SlurmExecutor(username=\"venkat\", address=\"beehive.agnostiq.ai\", poll_freq=20,\n",
    "    conda_env=\"covalent\", ssh_key_file=\"~/.ssh/id_ed25519\",\n",
    "    remote_workdir=\"/federation/venkat\",\n",
    "    options={\n",
    "        \"ntasks\": 1,\n",
    "        \"cpus-per-task\": 2,\n",
    "        \"partition\": \"debug\",\n",
    "        \"nodelist\": \"beehive-debug-st-t2medium-1\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    boston = load_boston()\n",
    "    data = pd.DataFrame(boston.data)\n",
    "    data['PRICE'] = boston.target\n",
    "    X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "    return X, y\n",
    "\n",
    "def split_dataset(features: pd.DataFrame, targets: pd.DataFrame):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def get_model(params: dict):\n",
    "    return xgb.XGBRegressor(**params)\n",
    "\n",
    "def train(model: xgb.XGBModel, features: np.ndarray, targets: np.ndarray):\n",
    "    model.fit(features, targets)\n",
    "    return model\n",
    "\n",
    "def predict(model: xgb.XGBModel, features: np.ndarray):\n",
    "    return model.predict(features)\n",
    "\n",
    "def measure_model_performance(targets: np.ndarray, predictions: np.ndarray):\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local execution withouth covalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset()\n",
    "X_train, X_test, y_train, y_test = split_dataset(X, y)\n",
    "model_to_train = get_model({\"objective\":\"reg:squarederror\",\n",
    "                                \"colsample_bytree\":0.3,\n",
    "                                \"learning_rate\": 0.1,\n",
    "                                \"max_depth\":5,\n",
    "                                \"alpha\": 10,\n",
    "                                \"n_estimators\": 10})\n",
    "trained_model= train(model_to_train, X_train, y_train)\n",
    "predictions = trained_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(\"RMSE: %f\"%(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def split_dataset(features: pd.DataFrame, targets: pd.DataFrame):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=123)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "@ct.electron\n",
    "def get_model(params: dict) -> xgb.XGBModel:\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    print(type(model))\n",
    "    return model\n",
    "\n",
    "@ct.electron\n",
    "def train(model: xgb.XGBModel, features, targets):\n",
    "    print(type(model))\n",
    "    model.fit(features, targets)\n",
    "    return model\n",
    "\n",
    "@ct.electron\n",
    "def predict(model: xgb.XGBModel, features: np.ndarray):\n",
    "    return model.predict(features)\n",
    "\n",
    "@ct.electron\n",
    "def measure_model_performance(targets: np.ndarray, predictions: np.ndarray):\n",
    "    return np.sqrt(mean_squared_error(targets, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def workflow(X, y):\n",
    "    X_train, X_test, y_train, y_test = split_dataset(features=X, targets=y)\n",
    "    model_to_train = get_model({\"objective\":\"reg:squarederror\",\n",
    "                                \"colsample_bytree\":0.3,\n",
    "                                \"learning_rate\": 0.1,\n",
    "                                \"max_depth\": 5,\n",
    "                                \"alpha\": 10,\n",
    "                                \"n_estimators\": 10})\n",
    "    trained_model = train(model=model_to_train, features=X_train, targets=y_train)\n",
    "    predictions = predict(model=trained_model, features=X_test)\n",
    "    score = measure_model_performance(targets=y_test, predictions=predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset()\n",
    "dispatch_id = ct.dispatch(workflow)(X, y)\n",
    "print(dispatch_id)\n",
    "result = ct.get_result(dispatch_id=dispatch_id, wait=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_tune={\n",
    "    \"colsample_bytree\": np.linspace(0.1, 0.5, 1),\n",
    "    \"learning_rate\": np.linspace(0.01, 0.1, 1),\n",
    "    \"max_depth\": [5],\n",
    "    \"alpha\": np.arange(5, 11, 1),\n",
    "    \"n_estimators\": np.arange(8, 11, 1)\n",
    "}\n",
    "\n",
    "# Get a list of all parameters to use to build a model and cross validate\n",
    "grid = list(ParameterGrid(params_to_tune))\n",
    "print(len(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def cross_validate_model(model_params: dict, features, target, n_folds: int):\n",
    "    model = xgb.XGBRegressor(**model_params)\n",
    "    cv_scores = cross_val_score(model, features, targets, scoring='neg_root_mean_squared_error', cv=n_folds)\n",
    "    return np.mean(cv_scores), np.std(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def workflow(parameters, X, y):\n",
    "    results = []\n",
    "    for p in parameters:\n",
    "        avg_score, score_std = cross_validate_model(model_params=p, features=X, targets=y, n_folds=3)\n",
    "        entry = {'params': p, 'avg_score': avg_score, 'score_std': score_std}\n",
    "        results.append(entry)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispatch_id = ct.dispatch(workflow)(grid, X_train, y_train)\n",
    "print(dispatch_id)\n",
    "#result = ct.get_result(dispatch_id=dispatch_id, wait=True)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list()\n",
    "scores_std = list()\n",
    "for p in grid:\n",
    "    model = xgb.XGBRegressor(objective=\"reg:squarederror\", **p)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=3)\n",
    "    scores.append(np.mean(cv_scores))\n",
    "    scores_std.append(np.std(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use covalent to dispatch to Slurm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study model performance as a function of the tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsample_bytree_values = []\n",
    "n_estimator_values = []\n",
    "learning_rate_values = []\n",
    "max_depth_values = []\n",
    "alpha_values = []\n",
    "for p in grid:\n",
    "    colsample_bytree_values.append(p['colsample_bytree'])\n",
    "    learning_rate_values.append(p['learning_rate'])\n",
    "    max_depth_values.append(p['max_depth'])\n",
    "    alpha_values.append(p['alpha'])\n",
    "    n_estimator_values.append(p['n_estimators'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group scores with colsample_bytree parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for colsample, score in zip(colsample_bytree_values, scores):\n",
    "    entry = {'colsample_bytree': colsample, 'score': score}\n",
    "    temp.append(entry)\n",
    "df = pd.DataFrame(temp)\n",
    "temp2 = []\n",
    "for unique in np.unique(colsample_bytree_values):\n",
    "    entry = {'colsample_bytree': unique, 'avg_score': np.mean(df[df['colsample_bytree'] == unique]['score'])}\n",
    "    temp2.append(entry)\n",
    "df_colsample_bytree_vs_scores = pd.DataFrame(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group scores with learning_rate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for lrate, score in zip(learning_rate_values, scores):\n",
    "    entry = {'learning_rate': lrate, 'score': score}\n",
    "    temp.append(entry)\n",
    "df = pd.DataFrame(temp)\n",
    "temp2 = []\n",
    "for unique in np.unique(learning_rate_values):\n",
    "    entry = {'learning_rate': unique, 'avg_score': np.mean(df[df['learning_rate'] == unique]['score'])}\n",
    "    temp2.append(entry)\n",
    "df_learning_rate_vs_scores = pd.DataFrame(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group scores with alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for alpha, score in zip(alpha_values, scores):\n",
    "    entry = {'alpha': alpha, 'score': score}\n",
    "    temp.append(entry)\n",
    "df = pd.DataFrame(temp)\n",
    "temp2 = []\n",
    "for unique in np.unique(alpha_values):\n",
    "    entry = {'alpha': unique, 'avg_score': np.mean(df[df['alpha'] == unique]['score'])}\n",
    "    temp2.append(entry)\n",
    "df_alpha_vs_scores = pd.DataFrame(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df_colsample_bytree_vs_scores['colsample_bytree'], df_colsample_bytree_vs_scores['avg_score'], 'o-', ms=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df_learning_rate_vs_scores['learning_rate'], df_learning_rate_vs_scores['avg_score'], 'o-', ms=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df_alpha_vs_scores['alpha'], df_alpha_vs_scores['avg_score'], 'o-', ms=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8d132f2c9868f0de786af45cb93a5aab017a81263068ec051dedbc1ccb266a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cova')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
