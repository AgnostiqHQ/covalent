{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "edcfb2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets, svm\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import covalent as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec97320",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_executor = ct.executor.AWSBatchExecutor(\n",
    "    profile=\"aq\",\n",
    "    region=\"us-east-1\",\n",
    "    batch_execution_role_name = \"ae-batch-aws-demo-task-execution-role\",\n",
    "    batch_job_definition_name = \"ae-batch-aws-demo-job-definition\",\n",
    "    batch_job_log_group_name = \"ae-batch-aws-demo-log-group\",\n",
    "    batch_job_role_name = \"ae-batch-aws-demo-job-role\",\n",
    "    batch_queue = \"ae-batch-aws-demo-queue\",\n",
    "    s3_bucket_name = \"ae-batch-aws-demo-job-resources\"\n",
    ")\n",
    "\n",
    "batch_deps_pip=ct.DepsPip(packages=[\"scikit-learn\",\"torch\",\"torchvision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e8644f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_executor = ct.executor.AWSLambdaExecutor(\n",
    "    profile=\"aq\",\n",
    "    region=\"us-east-1\",\n",
    "    function_name = \"ae-lambda-aws-demo-lambda-fn\",\n",
    "    s3_bucket_name = \"ae-lambda-aws-demo-covalent-artifact-bucket\",\n",
    ")\n",
    "\n",
    "lambda_deps_pip = ct.DepsPip(packages=[\"scikit-learn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1470886",
   "metadata": {},
   "outputs": [],
   "source": [
    "braket_executor = ct.executor.BraketExecutor(\n",
    "    profile=\"aq\",\n",
    "    region=\"us-east-1\",\n",
    "    braket_job_execution_role_name = \"amazon-braket-ae-aws-demo-role\",\n",
    "    ecr_image_uri = \"348041629502.dkr.ecr.us-east-1.amazonaws.com/amazon-braket-ae-aws-demo-base-executor-repo:latest\",\n",
    "    s3_bucket_name = \"amazon-braket-ae-aws-demo-bucket\",\n",
    "    time_limit=60*60\n",
    ")\n",
    "\n",
    "braket_deps_pip = ct.DepsPip(packages=[\"scikit-learn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4b0b4010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b116242a-f0f0-4d66-a18d-3ae09d9eba04\n"
     ]
    }
   ],
   "source": [
    "# Low compute\n",
    "@ct.electron\n",
    "def get_feature_combinations(num_features):\n",
    "    features = []\n",
    "    for length in range(num_features):\n",
    "        for comb in itertools.combinations(range(num_features), length):\n",
    "            features.append(comb)\n",
    "    return features[1:]\n",
    "\n",
    "\n",
    "# Sent to lambda\n",
    "@ct.electron(\n",
    "    executor=lambda_executor,\n",
    "    deps_pip=lambda_deps_pip\n",
    ")\n",
    "def fit_models_calculate_accuracy(data, model, features_to_select):\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    X_train = X_train[:, features_to_select]\n",
    "    model.fit(X_train, y_train)\n",
    "    return model, model.score(X_test[:, features_to_select], y_test)\n",
    "\n",
    "\n",
    "# Low compute\n",
    "@ct.electron\n",
    "def load_data(reduce_dims=12, num_data_points=None):\n",
    "    X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "    if num_data_points:\n",
    "        X = X[:num_data_points]\n",
    "        y = y[:num_data_points]\n",
    "    X = PCA(n_components=reduce_dims).fit_transform(X)\n",
    "    data = train_test_split(X, y, test_size=0.5)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Low compute\n",
    "@ct.electron\n",
    "def select_best_model(models_and_accuracies, feature_combinations):\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    best_position = 0\n",
    "    for i, (model, accuracy) in enumerate(models_and_accuracies):\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "            best_position = i\n",
    "    return best_model, feature_combinations[best_position]\n",
    "\n",
    "\n",
    "@ct.electron\n",
    "def find_best_features(data, model, feature_combinations):\n",
    "    models_and_accuracies = []\n",
    "    for feature_list in feature_combinations:\n",
    "        models_and_accuracies_tmp = fit_models_calculate_accuracy(data, model, feature_list)\n",
    "        models_and_accuracies.append(models_and_accuracies_tmp)\n",
    "    return select_best_model(models_and_accuracies, feature_combinations)\n",
    "\n",
    "\n",
    "# Low compute\n",
    "@ct.electron\n",
    "def plot_confusion_matrix(model, data, feature_list):\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    X_test = X_test[:, feature_list]\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, predictions, labels=model.classes_)\n",
    "    return ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "\n",
    "\n",
    "# Braket executor\n",
    "@ct.electron(\n",
    "    executor=braket_executor,\n",
    "    deps_pip=braket_deps_pip\n",
    ")\n",
    "def do_quantum_svm(data, features, ansatz):\n",
    "    device_arn = os.environ[\"AMZN_BRAKET_DEVICE_ARN\"]\n",
    "    s3_bucket = os.environ[\"AMZN_BRAKET_OUT_S3_BUCKET\"]\n",
    "    s3_task_dir = os.environ[\"AMZN_BRAKET_TASK_RESULTS_S3_URI\"].split(s3_bucket)[1]\n",
    "\n",
    "    device = qml.device(\n",
    "        \"braket.aws.qubit\",\n",
    "        device_arn=device_arn,\n",
    "        s3_destination_folder=(s3_bucket, s3_task_dir),\n",
    "        wires=features\n",
    "    )\n",
    "    \n",
    "    @qml.qnode(device)\n",
    "    def kernel(x1, x2):\n",
    "        ansatz(x1, wires=features)\n",
    "        qml.adjoint(ansatz)(x2, wires=features)\n",
    "        return qml.expval(qml.Projector(range(len(features)), features))\n",
    "\n",
    "    quantum_kernel = lambda x1, x2: np.array([[kernel(i, j).numpy() for i in x1] for j in x2])\n",
    "    model = svm.SVC(kernel=quantum_kernel)\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    X_train = X_train[:, features]\n",
    "    X_test = X_test[:, features]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "\n",
    "    return model, score\n",
    "\n",
    "\n",
    "# low compute\n",
    "@ct.electron\n",
    "def get_data(data, features, batch_size=4):\n",
    "    class Data(Dataset):\n",
    "        def __init__(self, X_train, y_train):\n",
    "            self.X = torch.from_numpy(X_train.astype(np.float32))\n",
    "            self.y = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "            self.len = self.X.shape[0]\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.X[index], self.y[index]\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.len\n",
    "\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    X_test = X_test[:, features]\n",
    "    X_train = X_train[:, features]\n",
    "    traindata = Data(X_train, y_train)\n",
    "    trainloader = DataLoader(traindata, batch_size=batch_size, shuffle=True)\n",
    "    return trainloader\n",
    "\n",
    "# Batch compute\n",
    "@ct.electron(\n",
    "    executor=batch_executor,\n",
    "    deps_pip=batch_deps_pip\n",
    ")\n",
    "def train_dnn(data, features, trainloader, hidden_layers=10, epochs=10, layers_size=5):\n",
    "    input_dim = len(features)\n",
    "    output_dim = trainloader.dataset.y.unique().shape[0]\n",
    "\n",
    "    def get_net(hidden_layers):\n",
    "        layers = [nn.Linear(input_dim, layers_size)]\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(layers_size, layers_size))\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(layers_size, output_dim),)\n",
    "        return layers\n",
    "\n",
    "    class Network(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Network, self).__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(*get_net(hidden_layers))\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.flatten(x)\n",
    "            x = self.linear_relu_stack(x)\n",
    "            return x\n",
    "\n",
    "    clf = Network()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(clf.parameters(), lr=0.05)\n",
    "\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data_train in enumerate(trainloader, 0):\n",
    "            inputs, labels = data_train\n",
    "            optimizer.zero_grad()\n",
    "            outputs = clf(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            losses.append(running_loss)\n",
    "\n",
    "    _, X_test, _, y_test = data\n",
    "    X_test = X_test[:, features]\n",
    "    _, prediction = torch.max(clf(torch.Tensor(X_test)), 1)\n",
    "    accuracy = (y_test == prediction.numpy()).sum() / y_test.shape[0]\n",
    "    return clf, accuracy\n",
    "\n",
    "\n",
    "@ct.electron\n",
    "def compare_classical_quantum(classical_accuracy, quantum_accuracy):\n",
    "    plt.switch_backend(\"Agg\")\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "    data = {\"x\": [\"Classical\", \"Quantum\"], \"y\": [classical_accuracy, quantum_accuracy]}\n",
    "    sns.barplot(x=\"x\", y=\"y\", data=data, ax=ax)\n",
    "    ax.bar_label(ax.containers[0])\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    plt.close()\n",
    "    return fig\n",
    "\n",
    "\n",
    "@ct.lattice\n",
    "def my_experiment(\n",
    "    num_data_points,\n",
    "    feature_reduction_model,\n",
    "    reduce_dims,\n",
    "    quantum_ansatz,\n",
    "    classical_hidden_layers,\n",
    "    classical_layers_size,\n",
    "    classical_epochs,\n",
    "):\n",
    "    data = load_data(reduce_dims=reduce_dims, num_data_points=num_data_points)\n",
    "    feature_combinations = get_feature_combinations(reduce_dims)\n",
    "    best_model, best_features = find_best_features(\n",
    "        data, feature_reduction_model, feature_combinations\n",
    "    )\n",
    "    plot_confusion_matrix(best_model, data, best_features)\n",
    "    quantum_svm_model, quantum_score = do_quantum_svm(data, best_features, quantum_ansatz)\n",
    "    train_loader = get_data(data, best_features)\n",
    "    classical_dnn, classical_score = train_dnn(\n",
    "        data,\n",
    "        best_features,\n",
    "        trainloader=train_loader,\n",
    "        hidden_layers=classical_hidden_layers,\n",
    "        layers_size=classical_layers_size,\n",
    "        epochs=classical_epochs,\n",
    "    )\n",
    "    compare_classical_quantum(classical_score, quantum_score)\n",
    "    return quantum_svm_model, quantum_score, classical_dnn, classical_score\n",
    "\n",
    "\n",
    "# number of points in dataset (increasing this will increase SVM memory as well as quantum SVM kernel computation time)\n",
    "num_data_points = 200  # None implies all data ~ 500\n",
    "# Controls the number of features which increases the number of lambdas\n",
    "reduce_dims = 3\n",
    "feature_reduction_model = svm.SVC()\n",
    "quantum_ansatz = qml.AngleEmbedding\n",
    "\n",
    "# Controls calssical batch processing power\n",
    "classical_hidden_layers = 3\n",
    "classical_layers_size = 3\n",
    "classical_epochs = 10\n",
    "\n",
    "runid = ct.dispatch(my_experiment)(\n",
    "    num_data_points=num_data_points,\n",
    "    feature_reduction_model=feature_reduction_model,\n",
    "    reduce_dims=reduce_dims,\n",
    "    quantum_ansatz=quantum_ansatz,\n",
    "    classical_hidden_layers=classical_hidden_layers,\n",
    "    classical_layers_size=classical_layers_size,\n",
    "    classical_epochs=classical_epochs,\n",
    ")\n",
    "print(runid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292cdaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('notebook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": ""
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
