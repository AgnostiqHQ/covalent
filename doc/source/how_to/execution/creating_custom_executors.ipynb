{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a custom executor\n",
    "\n",
    "<font size=\"3\">\n",
    "\n",
    "\n",
    "Each electron can utilise different so-called executors. These executors define the low-level directions for the computation. They can specify different capabilities, _eg_, different hardware, different computation strategy, different logic, or simply different goals.\n",
    "\n",
    "Executors are plugins; any executor-plugins which are found are imported as classes in the covalent.executor name-space.\n",
    "See the how-to on [choosing an executor to be used in an electron](choosing_executors.ipynb) for details on simply choosing an executor. Here, we go through the steps, line-by-line, on how to make your own.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "There is a certain amount of boiler-plate Covalent code and imports that are required:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# All executor plugins inherit from the BaseExecutor base class. The wrapper_fn is used to wrap the TransportableObject between the\n",
    "# call_before and call_after hooks\n",
    "from covalent.executor import BaseExecutor, wrapper_fn\n",
    "\n",
    "# The current status of the execution can be kept up-to-date with Covalent Result objects.\n",
    "from covalent._results_manager.result import Result\n",
    "\n",
    "# DispatchInfo objects are used to share info of a dispatched computation between different\n",
    "# tasks (electrons) of the workflow (lattice).\n",
    "from covalent._shared_files.util_classes import DispatchInfo\n",
    "\n",
    "# The function to be computed is in the form of a Covalent TransportableObject.\n",
    "# This import is not strictly necessary, as it is only used for type-hints.\n",
    "from covalent._workflow.transport import TransportableObject\n",
    "\n",
    "# The Covalent logger module is a simple wrapper around the\n",
    "# standard Python logging module.\n",
    "from covalent._shared_files import logger\n",
    "app_log = logger.app_log\n",
    "log_stack_info = logger.log_stack_info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "We also need a few required standard Python imports:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# These modules are used to capture print/logging statements\n",
    "# inside electron definitions.\n",
    "import io\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "\n",
    "# For type-hints\n",
    "from typing import Any, Dict, List\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Somewhere in the module, the module attribute \"EXECUTOR_PLUGIN_NAME\" must be set to the class name defining the executor:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# The plugin class name must be given by the EXECUTOR_PLUGIN_NAME attribute. In case this\n",
    "# module has more than one class defined, this lets Covalent know which is the executor class.\n",
    "EXECUTOR_PLUGIN_NAME = \"CustomExecutor\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Executor-specific inputs should have sane defaults defined in the global variable `_EXECUTOR_PLUGIN_DEFAULTS`. These defaults will be written to the user's Covalent configuration file, if they were not already present.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "_EXECUTOR_PLUGIN_DEFAULTS = {\n",
    "    \"executor_input1\": \"\",\n",
    "    \"executor_input2\": \"\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Now we define the executor plugin class. Input arguments to the executor class are not necessary, but in this example we have two, as well as other arbitrary arguments (*args) and keyword arguments (**kwargs).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class CustomExecutor(BaseExecutor):\n",
    "    def __init__(\n",
    "        # Inputs to an executor can be positional or keyword arguments.\n",
    "        self,\n",
    "        executor_input1: str,\n",
    "        executor_input2: int = 0,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "\n",
    "    ) -> None:\n",
    "        self.executor_input1 = executor_input1\n",
    "        self.executor_input2 = executor_input2\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        # Call the BaseExecutor initialization:\n",
    "        # There are a number of optional arguments to BaseExecutor\n",
    "        # that could be specfied as keyword inputs to CustomExecutor.\n",
    "        base_kwargs = {}\n",
    "        for key, _ in self.kwargs.items():\n",
    "            if key in [\n",
    "                \"conda_env\",\n",
    "                \"cache_dir\",\n",
    "                \"current_env_on_conda_fail\",\n",
    "            ]:\n",
    "                base_kwargs[key] = self.kwargs[key]\n",
    "\n",
    "        super().__init__(**base_kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "The \"execute\" class method is the main work-horse of the executor plugin. Here is the definition of where and how the input function will be computed.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def execute(\n",
    "        self,\n",
    "        function: TransportableObject,\n",
    "        args: List,\n",
    "        kwargs: Dict,\n",
    "        call_before: List,\n",
    "        call_after: List,\n",
    "        dispatch_id: str,\n",
    "        results_dir: str,\n",
    "        node_id: int = -1\n",
    "    ) -> Any:\n",
    "\n",
    "        \"\"\"\n",
    "        Execute the function with the given arguments. The `call_before` and `call_after` are a\n",
    "        list of callables that are executed before and after the function execution respectively\n",
    "\n",
    "        Args:\n",
    "            function: The input (serialized) python function which will be executed and\n",
    "                whose result is ultimately returned by this function.\n",
    "            args: List of positional arguments to be used by the function.\n",
    "            kwargs: Dictionary of keyword arguments to be used by the function.\n",
    "            call_before: List of Python callables to be executed before function executes.\n",
    "            call_after: List of Python callables to be executed after function executes.\n",
    "            dispatch_id: The unique identifier of the external lattice process which is calling the function.\n",
    "            results_dir: The location of the results directory.\n",
    "            node_id: The ID of this task/node in the workflow graph.\n",
    "\n",
    "        Returns:\n",
    "            output: The result of the executed function.\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Here, we convert the function from a Covalent TransportableObject to a \"standard\" Python function. From here, we are free to create whatever operations make this custom executor unique.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        # Convert the function to be executed, which is in the form of\n",
    "        # a Covalent TransportableObject, to a 'standard' Python function.\n",
    "        # Invoke the wrapper_fn to get a transportable object with call_before/call_after hooks placed properly\n",
    "        fn = wrapper_fn(function, call_before, call_after, args, kwargs).get_deserialized()\n",
    "        app_log.debug(type(fn))\n",
    "\n",
    "        # In this block is where operations specific to your custom executor\n",
    "        # can be defined. These operations could manipulate the function, the\n",
    "        # inputs/outputs, or anything that Python allows.\n",
    "        external_object = ExternalClass(3)\n",
    "        app_log.debug(external_object.multiplier)\n",
    "        \n",
    "        # Store the current status to this shared-between-processes object.\n",
    "        info_dict = {\"STATUS\": Result.RUNNING}\n",
    "        info_queue.put_nowait(info_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "The execution of the input function must be done within the following context manager:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        result = None\n",
    "        exception = None\n",
    "\n",
    "        with self.get_dispatch_context(DispatchInfo(dispatch_id)), redirect_stdout(\n",
    "            io.StringIO()\n",
    "        ) as stdout, redirect_stderr(io.StringIO()) as stderr:\n",
    "            # Here we simply execute the function on the local machine.\n",
    "            # But this could be sent to a more capable machine for the operation,\n",
    "            # or a different Python virtual environment, or more.\n",
    "            try:\n",
    "                result = fn(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                exception = e\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Other unique custom operations or informational logging can be done here as well.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        # Other custom operations can be applied here.\n",
    "        if result is not none:\n",
    "            result = self.helper_function(result)\n",
    "\n",
    "        debug_message = f\"Function '{fn.__name__}' was executed on node {node_id} with execution arguments {execution_args}\"\n",
    "        app_log.debug(debug_message)\n",
    "        \n",
    "        # Update the status:\n",
    "        info_dict = info_queue.get()\n",
    "        info_dict[\"STATUS\"] = Result.FAILED if result is None else Result.COMPLETED\n",
    "        info_queue.put(info_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Finally, return the result and any print statements or log/error messages from the execution, if any:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        return (result, stdout.getvalue(), stderr.getvalue(), exception)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Other class fucntions can, of course, be defined and used:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def helper_function(self, result):\n",
    "        return 2*result\n",
    "    \n",
    "    def get_status(self, info_dict: dict) -> Result:\n",
    "        \"\"\"\n",
    "        Get the current status of the task.\n",
    "\n",
    "        Args:\n",
    "            info_dict: a dictionary containing any neccessary parameters needed to query the\n",
    "                status. For this class (LocalExecutor), the only info is given by the\n",
    "                \"STATUS\" key in info_dict.\n",
    "\n",
    "        Returns:\n",
    "            A Result status object (or None, if \"STATUS\" is not in info_dict).\n",
    "        \"\"\"\n",
    "\n",
    "        return info_dict.get(\"STATUS\", Result.NEW_OBJ)\n",
    "    \n",
    "    def cancel(self, info_dict: dict = {}) -> Tuple[Any, str, str]:\n",
    "        \"\"\"\n",
    "        Cancel the execution task.\n",
    "\n",
    "        Args:\n",
    "            info_dict: a dictionary containing any neccessary parameters\n",
    "                needed to halt the task execution.\n",
    "\n",
    "        Returns:\n",
    "            Null values in the same structure as a successful return value (a 4-element tuple).\n",
    "        \"\"\"\n",
    "\n",
    "        return (None, \"\", \"\", InterruptedError)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Similarly, other classes can be in the same module:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# This class can be used in the custom executor, but will be ignored by the\n",
    "# plugin-loader, since it is not designated as the plugin class.\n",
    "class ExternalClass:\n",
    "\n",
    "    def __init__(self, multiplier: int):\n",
    "        self.multiplier = multiplier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Here is the entire module in one executable cell:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Agnostiq Inc.\n",
    "#\n",
    "# This file is part of Covalent.\n",
    "#\n",
    "# Licensed under the GNU Affero General Public License 3.0 (the \"License\").\n",
    "# A copy of the License may be obtained with this software package or at\n",
    "#\n",
    "#      https://www.gnu.org/licenses/agpl-3.0.en.html\n",
    "#\n",
    "# Use of this file is prohibited except in compliance with the License. Any\n",
    "# modifications or derivative works of this file must retain this copyright\n",
    "# notice, and modified files must contain a notice indicating that they have\n",
    "# been altered from the originals.\n",
    "#\n",
    "# Covalent is distributed in the hope that it will be useful, but WITHOUT\n",
    "# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n",
    "# FITNESS FOR A PARTICULAR PURPOSE. See the License for more details.\n",
    "#\n",
    "# Relief from the License may be granted by purchasing a commercial license.\n",
    "\n",
    "\"\"\"This is an example of a custom Covalent executor plugin.\"\"\"\n",
    "\n",
    "# The Covalent logger module is a simple wrapper around the\n",
    "# These modules are used to capture print/logging statements\n",
    "# inside electron definitions.\n",
    "import io\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "\n",
    "# For type-hints\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "# The current status of the execution can be kept up-to-date with Covalent Result objects.\n",
    "from covalent._results_manager.result import Result\n",
    "\n",
    "# Covalent logger\n",
    "from covalent._shared_files import logger\n",
    "\n",
    "# DispatchInfo objects are used to share info of a dispatched computation between different\n",
    "# tasks (electrons) of the workflow (lattice).\n",
    "from covalent._shared_files.util_classes import DispatchInfo\n",
    "\n",
    "# The function to be computed is in the form of a Covalent TransportableObject.\n",
    "# This import is not strictly necessary, as it is only used for type-hints.\n",
    "from covalent._workflow.transport import TransportableObject\n",
    "\n",
    "# All executor plugins inherit from the BaseExecutor base class.\n",
    "from covalent.executor import BaseExecutor, wrapper_fn\n",
    "\n",
    "app_log = logger.app_log\n",
    "log_stack_info = logger.log_stack_info\n",
    "\n",
    "# The plugin class name must be given by the EXECUTOR_PLUGIN_NAME attribute. In case this\n",
    "# module has more than one class defined, this lets Covalent know which is the executor class.\n",
    "EXECUTOR_PLUGIN_NAME = \"CustomExecutor\"\n",
    "\n",
    "_EXECUTOR_PLUGIN_DEFAULTS = {\n",
    "    \"executor_input1\": \"\",\n",
    "    \"executor_input2\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "class CustomExecutor(BaseExecutor):\n",
    "    def __init__(\n",
    "        # Inputs to an executor can be positional or keyword arguments.\n",
    "        self,\n",
    "        executor_input1: str,\n",
    "        executor_input2: int = 0,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.executor_input1 = executor_input1\n",
    "        self.executor_input2 = executor_input2\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        # Call the BaseExecutor initialization:\n",
    "        # There are a number of optional arguments to BaseExecutor\n",
    "        # that could be specfied as keyword inputs to CustomExecutor.\n",
    "        base_kwargs = {}\n",
    "        for key, _ in self.kwargs.items():\n",
    "            if key in [\n",
    "                \"conda_env\",\n",
    "                \"cache_dir\",\n",
    "                \"current_env_on_conda_fail\",\n",
    "            ]:\n",
    "                base_kwargs[key] = self.kwargs[key]\n",
    "\n",
    "        super().__init__(**base_kwargs)\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        function: TransportableObject,\n",
    "        args: List,\n",
    "        kwargs: Dict,\n",
    "        call_before: List,\n",
    "        call_after: List,\n",
    "        dispatch_id: str,\n",
    "        results_dir: str,\n",
    "        node_id: int = -1\n",
    "    ) -> Any:\n",
    "\n",
    "        \"\"\"\n",
    "        Executes the input function and returns the result.\n",
    "\n",
    "        Args:\n",
    "            function: The input (serialized) python function which will be executed and\n",
    "                whose result is ultimately returned by this function.\n",
    "            args: List of positional arguments to be used by the function.\n",
    "            kwargs: Dictionary of keyword arguments to be used by the function.\n",
    "            call_before: List of Python callables to be executed before function executes.\n",
    "            call_after: List of Python callables to be executed after function executes.\n",
    "            dispatch_id: The unique identifier of the external lattice process which is calling the function.\n",
    "            results_dir: The location of the results directory.\n",
    "            node_id: The ID of this task/node in the workflow graph.\n",
    "\n",
    "        Returns:\n",
    "            output: The result of the executed function.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert the function to be executed, which is in the form of\n",
    "        # a Covalent TransportableObject, to a 'standard' Python function:\n",
    "\n",
    "        fn = wrapper_fn(function, call_before, call_after, args, kwargs).get_deserialized()\n",
    "        app_log.debug(type(fn))\n",
    "\n",
    "        # In this block is where operations specific to your custom executor\n",
    "        # can be defined. These operations could manipulate the function, the\n",
    "        # inputs/outputs, or anything that Python allows.\n",
    "        external_object = ExternalClass(3)\n",
    "        app_log.debug(external_object.multiplier)\n",
    "\n",
    "        result = None\n",
    "        exception = None\n",
    "\n",
    "        with self.get_dispatch_context(DispatchInfo(dispatch_id)), redirect_stdout(\n",
    "            io.StringIO()\n",
    "        ) as stdout, redirect_stderr(io.StringIO()) as stderr:\n",
    "            # Here we simply execute the function on the local machine.\n",
    "            # But this could be sent to a more capable machine for the operation,\n",
    "            # or a different Python virtual environment, or more.\n",
    "            try:\n",
    "                result = fn(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                exception = e\n",
    "\n",
    "        # Other custom operations can be applied here.\n",
    "        if result is not None:\n",
    "            result = self.helper_function(result)\n",
    "\n",
    "        debug_message = f\"Function '{fn.__name__}' was executed on node {node_id}\"\n",
    "        app_log.debug(debug_message)\n",
    "\n",
    "        return (result, stdout.getvalue(), stderr.getvalue(), exception)\n",
    "\n",
    "    def helper_function(self, result):\n",
    "        \"\"\"An example helper function.\"\"\"\n",
    "\n",
    "        return 2 * result\n",
    "\n",
    "    def get_status(self, info_dict: dict) -> Result:\n",
    "        \"\"\"\n",
    "        Get the current status of the task.\n",
    "\n",
    "        Args:\n",
    "            info_dict: a dictionary containing any neccessary parameters needed to query the\n",
    "                status. For this class (LocalExecutor), the only info is given by the\n",
    "                \"STATUS\" key in info_dict.\n",
    "\n",
    "        Returns:\n",
    "            A Result status object (or None, if \"STATUS\" is not in info_dict).\n",
    "        \"\"\"\n",
    "\n",
    "        return info_dict.get(\"STATUS\", Result.NEW_OBJ)\n",
    "\n",
    "    def cancel(self, info_dict: dict = {}) -> Tuple[Any, str, str]:\n",
    "        \"\"\"\n",
    "        Cancel the execution task.\n",
    "\n",
    "        Args:\n",
    "            info_dict: a dictionary containing any neccessary parameters\n",
    "                needed to halt the task execution.\n",
    "\n",
    "        Returns:\n",
    "            Null values in the same structure as a successful return value (a 4-element tuple).\n",
    "        \"\"\"\n",
    "\n",
    "        return (None, \"\", \"\", InterruptedError)\n",
    "\n",
    "\n",
    "# This class can be used in the custom executor, but will be ignored by the\n",
    "# plugin-loader, since it is not designated as the plugin class.\n",
    "class ExternalClass:\n",
    "    \"\"\"An example external class.\"\"\"\n",
    "\n",
    "    def __init__(self, multiplier: int):\n",
    "        self.multiplier = multiplier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Using the aforementioned steps, let's create a Docker executor that can be used in workflows to execute electrons inside containers. NOTE: Covalent already has a `DockerExecutor` plugin that can be installed with `covalent` via `pip install covalent-docker-plugin`.\n",
    "\n",
    "We will implement this executor in stages in order to better illustrate the steps invovled and as well as to facilitate understanding. The idea behind creating a `DockerExecutor` is to allow user's to run their electrons in a containerized environment. As a starting step, we first create a docker image that we will use later to launch containers and run electrons within. We use the following `Dockerfile` for this purpose and install the necessary dependencies within.\n",
    "\n",
    "```docker\n",
    "# Dockerfile\n",
    "FROM python:3.8-buster\n",
    "\n",
    "RUN pip install cloudpickle && \\\n",
    "    pip install covalent\n",
    "\n",
    "CMD [\"/bin/bash\"]\n",
    "```\n",
    "\n",
    "In this Dockerfile, we install the bare minimum dependencies required to execute electrons. Next we build an image `docker-executor-demo:latest` from the Dockerfile template to be used in our workflows.\n",
    "\n",
    "```shell\n",
    "docker build -f Dockerfile --tag docker-executor-demo:latest .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach we intend to take in developing this executor is the following:\n",
    "\n",
    "* When the executor's execute method is called, we create a container using the `image` provided by the user as part of executor instantiation\n",
    "* As part of the container run command, we bind mount the user specified working directory into the container's working directory in `rw` mode. This is to allow retriving of any files generated within the container as part of the function execution to be accessible from outside of it before/during and after container run\n",
    "* Once the container is running, we using `docker container exec` to execute the pickled function object inside the container\n",
    "* Assert the `result` file gets created in the user's working directory as an artifact, if not raise an exception\n",
    "* Read the result object from the generated result file into `result`\n",
    "* Stop the container\n",
    "* Return `result`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1\n",
    "\n",
    "We can now being writing our `DockerExecutor` by first importing the boilerplate as well as any other dependencies needed by the executor.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "import cloudpickle as pickle\n",
    "import tempfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "from covalent.executor import BaseExecutor, wrapper_fn\n",
    "from covalent._shared_files.util_classes import DispatchInfo\n",
    "from covalent._shared_files import logger\n",
    "from covalent._workflow.transport import TransportableObject\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "\n",
    "To enable logging, we use the loggers present in Covalent itself\n",
    "\n",
    "```python\n",
    "app_log = logger.app_log\n",
    "log_stack_info = logger.log_stack_info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3\n",
    "\n",
    "We now specify any default configuration options for this executor. Note that specifying the configuration is entirely dependent on the author the the executor, as they can choose to expose any number of configuration options for the executors here.\n",
    "\n",
    "```python\n",
    "_EXECUTOR_PLUGIN_DEFAULTS = {\n",
    "    \"image\": \"python:3.8\"   # Default docker image to be used if not specified\n",
    "    \"workdir\": \".\" # Path to the working directory on disk where files generated during execution will be        stored. Defaults to current working directory\n",
    "    \"options\": {}    # Any options supported by the docker run CLI\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "\n",
    "Set the executor plugin name\n",
    "\n",
    "```python\n",
    "executor_plugin_name = 'DockerExecutor'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5\n",
    "\n",
    "Create the `DockerExecutor` class while deriving it from the `BaseExecutor` imported earlier.\n",
    "\n",
    "\n",
    "```python\n",
    "class DockerExecutor(BaseExecutor):\n",
    "    \"\"\"Docker executor plugin class\n",
    "    \n",
    "    Args:\n",
    "        :param str image: Name of the docker image to be used for running electrons.\n",
    "        :param str workdir: Path on the disk where files generated during execution will be created/stored.\n",
    "        :param dict options: Python dictionary of keyword arguments of the different options supported by the `docker run` CLI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        image: str,\n",
    "        workdir: str,\n",
    "        options: Dict = {}\n",
    "        container_workdir: str = \"/tmp/covalent\",  # Workdir inside the container\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        self.image = image\n",
    "        self.wordir = workdir\n",
    "        self.options = options\n",
    "        self._container_workdir = container_workdir\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6\n",
    "\n",
    "As seen from the outline, the executor is quite simple in the sense that it starts a container using th image provided by the user, bind mounts the working directory into the container, executes a command inside the running container and returns the result.\n",
    "\n",
    "The command to be executed can be rendered as a multiline string. To this end, we create a method `_format_exec_command` in our `DockerExecutor` to facilitate generation of the `exec` command\n",
    "\n",
    "```python\n",
    "def _format_exec_command(self, func_filename: str, result_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a python string that can be used to execute the task inside the container\n",
    "\n",
    "    Args:\n",
    "        :param str func_filename: Name of the pickle file from which to read the function to be executed.\n",
    "        :param str result_filename: Name of the pickle file into which the serialize and write the task result\n",
    "\n",
    "    Returns:\n",
    "        :param str script: Python string that can be parsed to execute the function\n",
    "    \"\"\"\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            \"import cloudpickle as pickle\",\n",
    "            \"\",\n",
    "            \"result = None\",\n",
    "            \"exception = None\",\n",
    "            \"\",\n",
    "            \"with open('{func_filename}', 'rb') as f:\",\n",
    "            \"   function, args, kwargs = pickle.load(f)\",\n",
    "            \"   try:\",\n",
    "            \"       result = function(*args, **kwargs)\",\n",
    "            \"   except Exception as e:\",\n",
    "            \"       exception = e\",\n",
    "            \"   finally:\",\n",
    "            \"       with open('{result_filename}', 'wb') as f_out:\",\n",
    "            \"           pickle.dump((result, exception), f_out)\",\n",
    "            \"\",\n",
    "        ]\n",
    "    ).format(\n",
    "        func_filename = os.path.join(self._container_workdir, func_filename),\n",
    "        result_filename = os.path.join(self._container_workdir, result_filename)\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7\n",
    "\n",
    "With the `_format_exec_command` helper method defined, we can now implement the executor's `execute` method that contains the core logic\n",
    "\n",
    "```python\n",
    "def execute(self,\n",
    "    function: TransportableObject,\n",
    "    args: List,\n",
    "    kwargs: List,\n",
    "    call_before: List,\n",
    "    call_after: List,\n",
    "    dispatch_id: str,\n",
    "    results_dir: str,\n",
    "    node_id: int - 1\n",
    "):\n",
    "    \"\"\"\n",
    "        Execute the callable inside the respective container and return the result\n",
    "\n",
    "        Args:\n",
    "            :param Callable function: The input python function to be executed inside the container whose result is to be returned back\n",
    "            :param List args: List of positional arguments required by the function\n",
    "            :param Dict kwargs: Dictionary of keyword arguments required by the function\n",
    "            :param List call_before: List of callables that will be invoked before the actual function execution\n",
    "            :param List  call_after: List of callables to be invoked after the function has finished execution\n",
    "            :param str dispatch_id: The unique identifier of the parent workflow that this electron is a part of\n",
    "            :param str results_dir: The location where the results from the function execution ought to be saved. This will be mounted inside the container\n",
    "            :param int node_id: ID of the node in the transport graph which is using this executor\n",
    "    \"\"\"\n",
    "\n",
    "    # set the necessary variables such as filenames to be used as part of the execution.\n",
    "    # Use the dispatch_id and the node_id in order to uniquely identify each file/container being\n",
    "    # created during execution\n",
    "    dispatch_info = DispatchInfo(dispatch_id=dispatch_id)\n",
    "    result_filename = f\"result-{dispatch_id}-{node_id}.pkl\"\n",
    "    container_name = f\"container-{dispatch_id}-{node_id}\"\n",
    "    execute_script_name = f\"dkrexec-{dispatch_id}-{node_id}.py\"\n",
    "\n",
    "    with self.get_dispatch_context(dispatch_info), tempfile.NamedTemporaryFile(\n",
    "            dir=self.workdir) as f, tempfile.NamedTemporaryFile(dir=self.workdir, mode=\"w\") as g:\n",
    "\n",
    "            # Wrap the `function` between the call_before and call_after hooks\n",
    "            new_args = [function, call_before, call_after]\n",
    "            for arg in args:\n",
    "                new_args.append(arg)\n",
    "\n",
    "            pickle.dump((wrapper_fn, new_args, kwargs), f)\n",
    "            f.flush()\n",
    "\n",
    "            # Format the command to be executed inside the container\n",
    "            func_filename = f\"func-{dispatch_id}-{node_id}.pkl\"\n",
    "            shutil.copy(f.name, os.path.join(self.workdir, func_filename))\n",
    "\n",
    "            cmd = self.format_exec_command(func_filename, result_filename)\n",
    "            g.write(cmd)\n",
    "            g.flush()\n",
    "            shutil.copy(g.name, os.path.join(self.workdir, execute_script_name))\n",
    "\n",
    "            # Start the container in detached mode\n",
    "            subprocess.run(\n",
    "                [\n",
    "                    \"docker\",\n",
    "                    \"container\",\n",
    "                    \"run\",\n",
    "                    \"-dit\",\n",
    "                     \"--rm\",\n",
    "                    \"--mount\",\n",
    "                    f\"type=bind,source={self.workdir},target={self._container_workdir}\",\n",
    "                    \"--name\",\n",
    "                    container_name,\n",
    "                    self.image\n",
    "                ],\n",
    "                check=True,\n",
    "                capture_output=True,\n",
    "            )\n",
    "\n",
    "            # Execute the script/command inside the container\n",
    "            subprocess.run(\n",
    "                [\n",
    "                    \"docker\",\n",
    "                    \"container\",\n",
    "                    \"exec\",\n",
    "                    container_name,\n",
    "                    \"python\",\n",
    "                    f\"{self._container_workdir}/{execute_script_name}\"\n",
    "                ],\n",
    "                check=True,\n",
    "                capture_output=True\n",
    "            )\n",
    "\n",
    "            # Assert that a result object was created\n",
    "            assert os.path.exists(os.path.join(self.workdir, result_filename))\n",
    "\n",
    "            # Read the generated result object from the result pickle file\n",
    "            with open(os.path.join(self.workdir, result_filename), \"rb\") as f_read:\n",
    "                result, exception = pickle.load(f_read)\n",
    "                if exception:\n",
    "                    raise exception\n",
    "\n",
    "            # Terminate the container\n",
    "            subprocess.run(\n",
    "                [\n",
    "                    \"docker\",\n",
    "                    \"container\",\n",
    "                    \"stop\",\n",
    "                    container_name\n",
    "                ],\n",
    "                check=True,\n",
    "                capture_output=True\n",
    "            )\n",
    "\n",
    "            # Return\n",
    "            return result, exception, None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, here is the entire executor as part of a single executable block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "import cloudpickle as pickle\n",
    "import tempfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "from covalent.executor import BaseExecutor, wrapper_fn\n",
    "from covalent._shared_files.util_classes import DispatchInfo\n",
    "from covalent._shared_files import logger\n",
    "from covalent._workflow.transport import TransportableObject\n",
    "\n",
    "app_log = logger.app_log\n",
    "log_stack_info = logger.log_stack_info\n",
    "\n",
    "_EXECUTOR_PLUGIN_DEFAULTS = {\n",
    "    \"image\": \"python:3.8\"   # Default docker image to be used if not specified\n",
    "    \"workdir\": \".\" # Path to the working directory on disk where files generated during execution will be        stored. Defaults to current working directory\n",
    "    \"options\": {}    # Any options supported by the docker run CLI\n",
    "}\n",
    "\n",
    "executor_plugin_name = 'DockerExecutor'\n",
    "\n",
    "class DockerExecutor(BaseExecutor):\n",
    "    \"\"\"Docker executor plugin class\n",
    "    \n",
    "    Args:\n",
    "        :param str image: Name of the docker image to be used for running electrons.\n",
    "        :param str workdir: Path on the disk where files generated during execution will be created/stored.\n",
    "        :param dict options: Python dictionary of keyword arguments of the different options supported by the `docker run` CLI.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        image: str,\n",
    "        workdir: str,\n",
    "        options: Dict = {}\n",
    "        container_workdir: str = \"/tmp/covalent\",  # Workdir inside the container\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        self.image = image\n",
    "        self.wordir = workdir\n",
    "        self.options = options\n",
    "        self._container_workdir = container_workdir\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _format_exec_command(self, func_filename: str, result_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a python string that can be used to execute the task inside the container\n",
    "\n",
    "    Args:\n",
    "        :param str func_filename: Name of the pickle file from which to read the function to be executed.\n",
    "        :param str result_filename: Name of the pickle file into which the serialize and write the task result\n",
    "\n",
    "    Returns:\n",
    "        :param str script: Python string that can be parsed to execute the function\n",
    "    \"\"\"\n",
    "        return \"\\n\".join(\n",
    "            [\n",
    "                \"import cloudpickle as pickle\",\n",
    "                \"\",\n",
    "                \"result = None\",\n",
    "                \"exception = None\",\n",
    "                \"\",\n",
    "                \"with open('{func_filename}', 'rb') as f:\",\n",
    "                \"   function, args, kwargs = pickle.load(f)\",\n",
    "                \"   try:\",\n",
    "                \"       result = function(*args, **kwargs)\",\n",
    "                \"   except Exception as e:\",\n",
    "                \"       exception = e\",\n",
    "                \"   finally:\",\n",
    "                \"       with open('{result_filename}', 'wb') as f_out:\",\n",
    "                \"           pickle.dump((result, exception), f_out)\",\n",
    "                \"\",\n",
    "            ]\n",
    "        ).format(\n",
    "            func_filename = os.path.join(self._container_workdir, func_filename),\n",
    "            result_filename = os.path.join(self._container_workdir, result_filename)\n",
    "        )\n",
    "\n",
    "\n",
    "    def execute(self,\n",
    "        function: TransportableObject,\n",
    "        args: List,\n",
    "        kwargs: List,\n",
    "        call_before: List,\n",
    "        call_after: List,\n",
    "        dispatch_id: str,\n",
    "        results_dir: str,\n",
    "        node_id: int - 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Execute the callable inside the respective container and return the result\n",
    "    \n",
    "            Args:\n",
    "                :param Callable function: The input python function to be executed inside the container whose result is to be returned back\n",
    "                :param List args: List of positional arguments required by the function\n",
    "                :param Dict kwargs: Dictionary of keyword arguments required by the function\n",
    "                :param List call_before: List of callables that will be invoked before the actual function execution\n",
    "                :param List  call_after: List of callables to be invoked after the function has finished execution\n",
    "                :param str dispatch_id: The unique identifier of the parent workflow that this electron is a part of\n",
    "                :param str results_dir: The location where the results from the function execution ought to be saved. This will be mounted inside the container\n",
    "                :param int node_id: ID of the node in the transport graph which is using this executor\n",
    "        \"\"\"\n",
    "    \n",
    "        # set the necessary variables such as filenames to be used as part of the execution.\n",
    "        # Use the dispatch_id and the node_id in order to uniquely identify each file/container being\n",
    "        # created during execution\n",
    "        dispatch_info = DispatchInfo(dispatch_id=dispatch_id)\n",
    "        result_filename = f\"result-{dispatch_id}-{node_id}.pkl\"\n",
    "        container_name = f\"container-{dispatch_id}-{node_id}\"\n",
    "        execute_script_name = f\"dkrexec-{dispatch_id}-{node_id}.py\"\n",
    "    \n",
    "        with self.get_dispatch_context(dispatch_info), tempfile.NamedTemporaryFile(\n",
    "                dir=self.workdir) as f, tempfile.NamedTemporaryFile(dir=self.workdir, mode=\"w\") as g:\n",
    "    \n",
    "                # Wrap the `function` between the call_before and call_after hooks\n",
    "                new_args = [function, call_before, call_after]\n",
    "                for arg in args:\n",
    "                    new_args.append(arg)\n",
    "    \n",
    "                pickle.dump((wrapper_fn, new_args, kwargs), f)\n",
    "                f.flush()\n",
    "    \n",
    "                # Format the command to be executed inside the container\n",
    "                func_filename = f\"func-{dispatch_id}-{node_id}.pkl\"\n",
    "                shutil.copy(f.name, os.path.join(self.workdir, func_filename))\n",
    "    \n",
    "                cmd = self.format_exec_command(func_filename, result_filename)\n",
    "                g.write(cmd)\n",
    "                g.flush()\n",
    "                shutil.copy(g.name, os.path.join(self.workdir, execute_script_name))\n",
    "    \n",
    "                # Start the container in detached mode\n",
    "                subprocess.run(\n",
    "                    [\n",
    "                        \"docker\",\n",
    "                        \"container\",\n",
    "                        \"run\",\n",
    "                        \"-dit\",\n",
    "                         \"--rm\",\n",
    "                        \"--mount\",\n",
    "                        f\"type=bind,source={self.workdir},target={self._container_workdir}\",\n",
    "                        \"--name\",\n",
    "                        container_name,\n",
    "                        self.image\n",
    "                    ],\n",
    "                    check=True,\n",
    "                    capture_output=True,\n",
    "                )\n",
    "    \n",
    "                # Execute the script/command inside the container\n",
    "                subprocess.run(\n",
    "                    [\n",
    "                        \"docker\",\n",
    "                        \"container\",\n",
    "                        \"exec\",\n",
    "                        container_name,\n",
    "                        \"python\",\n",
    "                        f\"{self._container_workdir}/{execute_script_name}\"\n",
    "                    ],\n",
    "                    check=True,\n",
    "                    capture_output=True\n",
    "                )\n",
    "    \n",
    "                # Assert that a result object was created\n",
    "                assert os.path.exists(os.path.join(self.workdir, result_filename))\n",
    "    \n",
    "                # Read the generated result object from the result pickle file\n",
    "                with open(os.path.join(self.workdir, result_filename), \"rb\") as f_read:\n",
    "                    result, exception = pickle.load(f_read)\n",
    "                    if exception:\n",
    "                        raise exception\n",
    "    \n",
    "                # Terminate the container\n",
    "                subprocess.run(\n",
    "                    [\n",
    "                        \"docker\",\n",
    "                        \"container\",\n",
    "                        \"stop\",\n",
    "                        container_name\n",
    "                    ],\n",
    "                    check=True,\n",
    "                    capture_output=True\n",
    "                )\n",
    "    \n",
    "        # Return\n",
    "        return result, exception, None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This executor can now be used as part of workflows to execute electons inside Docker containers after being properly imported. Following is a quick example\n",
    "\n",
    "```python\n",
    "import os\n",
    "import covalent as ct\n",
    "from covalent.executor import DockerExecutor\n",
    "\n",
    "docker = DockerExecutor(image='docker-executor-demo:latest', workdir=os.getcwd())\n",
    "\n",
    "@ct.electron(executor=docker)\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "@ct.electron(executor=docker)\n",
    "def multiply(x, y):\n",
    "    return x*y\n",
    "\n",
    "\n",
    "@ct.lattice\n",
    "def workflow(x, y):\n",
    "    r1 = add(x, y)\n",
    "    return multiply(y, r1)\n",
    "\n",
    "result = ct.dispatch_sync(workflow)(2, 3)\n",
    "print(result)\n",
    "\n",
    "\n",
    "Lattice Result\n",
    "==============\n",
    "status: COMPLETED\n",
    "result: 15\n",
    "inputs: {'args': [2, 3], 'kwargs': {}}\n",
    "error: None\n",
    "\n",
    "start_time: 2022-07-18 16:41:48.106224+00:00\n",
    "end_time: 2022-07-18 16:41:55.053115+00:00\n",
    "\n",
    "results_dir: results\n",
    "dispatch_id: 865fb0fd-7be3-4d15-a24d-97e5327b4253\n",
    "\n",
    "Node Outputs\n",
    "------------\n",
    "add(0): 5\n",
    ":parameter:2(1): 2\n",
    ":parameter:3(2): 3\n",
    "multiply(3): 15\n",
    ":parameter:3(4): 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cova-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": ""
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
